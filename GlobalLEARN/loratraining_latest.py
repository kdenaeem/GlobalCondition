# -*- coding: utf-8 -*-
"""LoraTraining.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1l-JBypqf-Fh93uKWRAp42mtOy6bgV3nL

### Finetuning Local LM using LoRa
Using lora to finetune local LM using summary_data.csv

# Set up
hugging face and pip install
"""

!huggingface-cli login



!pip3 install lxml[html_clean]
!pip3 install newspaper3k

"""## Model config
Configure LoRa and prepare model
Start here
"""

from peft import LoraConfig, get_peft_model
from transformers import AutoModelForCausalLM, AutoTokenizer
import pandas as pd
import torch
import os

# Define constants once
model_id = "meta-llama/Llama-3.2-1B-Instruct"
token = "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"

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_id, token=token, padding_side="left")

# Configure LoRA
lora_config = LoraConfig(
    task_type="CAUSAL_LM",
    r=64,
    lora_alpha=16,
    lora_dropout=0.1,
    target_modules=['q_proj', 'v_proj']
)

# Load model and apply LoRA config
model = AutoModelForCausalLM.from_pretrained(model_id, token=token)
model = get_peft_model(model, lora_config)
model.print_trainable_parameters()

def save_processed_dataset(data, filepath):
    """
    Save the processed DataFrame to a CSV file in Google Drive
    """
    print(f"Saving processed dataset to {filepath}")
    data.to_csv(filepath, index=False)
    print(f"Dataset saved successfully to {filepath}")

def load_processed_dataset(filepath):
    """
    Load a processed DataFrame from a CSV file in Google Drive
    """
    if os.path.exists(filepath):
        print(f"Loading processed dataset from {filepath}")
        return pd.read_csv(filepath)
    else:
        print(f"File not found at {filepath}")
        return None
complete_data = load_processed_dataset("/content/drive/MyDrive/AFiles/processed_data.csv")



"""## Preprocessing the News Summary dataset
Converting into a pandas dataframe and fetching news article content
"""

import os

# List files in the current directory to see what's available
print("Files in current directory:")
print(os.listdir())
import sys
import os
# Add the directory containing your module to the Python path
sys.path.append('/content/drive/MyDrive/AFiles')
from openailabeling import NewsSignificanceCalculator



# prompt: create a dataframe with a train and test set using summary_data.csv dataset I have saved in the folder
from openailabeling import NewsSignificanceCalculator
import pandas as pd
from sklearn.model_selection import train_test_split
import os
print(os.getcwd())

# Load the dataset
random_seed = 42

def prepare_news_dataset(input_file, output_file=None):
  """Preparing for finetuning"""
  print(f"loading dataset from {input_file}")
  df = pd.read_csv(input_file)

  required_columns = ['title', 'url']
  score_columns = ['scale', 'impact', 'potential', 'legacy', 'novelty', 'credibility', 'positivity']

  missing_columns = [col for col in required_columns
                     if col not in df.columns]
  if missing_columns:
    raise ValueError(f"Missing required columns: {missing_columns}")

  # convert score columns to numeric values
  for col in score_columns:
    df[col] = pd.to_numeric(df[col], errors='coerce')

  for col in score_columns:
    if df[col].isna().any():
      print(f"Warning column '{col}' contains missing values")
      df[col] = df[col].fillna(df[col].mean())

  if 'final_score' not in df.columns:
    print("Calculating final score as mean of all score components")
    df['final_score'] = df[score_columns].mean(axis=1)

  nsc = NewsSignificanceCalculator()
  return df

#shrink dataset for experimenting
def shrink_dataset(df: pd.DataFrame):
  df_shuffled = df.sample(frac=1, random_state=random_seed).reset_index(drop=True)
  df_shrinked = df_shuffled.head(200)
  return df_shrinked


def get_content(dataset):
  nsc = NewsSignificanceCalculator()
  if 'content' not in dataset.columns:
    print("Content column not found creating content")
    print("Downloading content from newsarticle")
    for i, row in dataset.iterrows():
      try:
        article_content = nsc.extract_article_content(row['url']).get('text', '')
        dataset.at[i, 'url'] = article_content
        if i % 5 == 0:
          print(f"Downloaded {i} articles")
      except Exception as e:
        print(f"Error downloading article {row['url']}: {e}")
        continue
    dataset = dataset.rename(columns={'url':'content'})
    return dataset


processed_df = prepare_news_dataset("/content/drive/MyDrive/AFiles/summary_data.csv")
dataset_sm = shrink_dataset(processed_df)
complete_data = get_content(dataset_sm)
df_train, df_test = train_test_split(complete_data, test_size=0.2, random_state=42, stratify=complete_data['category'] if 'category' in complete_data.columns else None)
complete_data = df_train
print(complete_data.head())



print(complete_data.head)

# Define the system prompt explaining the task and output format
SYSTEM_PROMPT = {
    "role": "system",
    "content": """Return a score of each factor for the news article I have attached at the end (0-10 for each factor):

Factors and weights:
- scale: Global population impact (10=affects all humanity, 5=regional, 2=local)
- impact: Immediate effect strength on humanity's development
- potential: Future influence on human civilization
- legacy: Historical milestone for human progress
- novelty: Uniqueness in human history
- credibility: Source reliability
- positivity: Positive development for humanity

Scale scoring guideline:
10: Affects all of humanity directly
8: Major global impact
6: Multi-region impact
4: Regional impact
2: Local impact

Impact scoring guideline:
10: Immediate global emergency
8: Major global change
6: Significant regional change
4: Moderate regional effect
2: Local effect

Return a JSON object with scores for the following news article. Use this format
and explain your reasoning for each score:
{
    "scale": N,
    "impact": N,
    "potential": N,
    "legacy": N,
    "novelty": N,
    "credibility": N,
    "positivity": N
}

Calibration Examples:
Global Impact (6.3/10): "Earth surpasses 1.5Â°C warming limit"
{
    "scale": 9,
    "impact": 8,
    "potential": 9,
    "legacy": 8,
    "novelty": 7,
    "credibility": 9,
    "positivity": 2
}

Regional Conflict (5.0/10): "Military escalation in ongoing war"
{
    "scale": 5,
    "impact": 6,
    "potential": 5,
    "legacy": 5,
    "novelty": 5,
    "credibility": 9,
    "positivity": 2
}

Local Event (2.2/10): "Regional sports championship"
{
    "scale": 2,
    "impact": 2,
    "potential": 1,
    "legacy": 1,
    "novelty": 2,
    "credibility": 8,
    "positivity": 8
}

Significance ranges:
High (6+): Major impact on human civilization
Medium (3-5): Regional/industry significance
Low (1-2): Local/minor impact
Score based on the following news article:"""
}

# Define the post message to guide the model's response format
POST_MESSAGE = {
    "role": "assistant",
    "content": "{"
}

# Define the score columns for consistent reference
SCORE_COLUMNS = ['scale', 'impact', 'potential', 'legacy', 'novelty', 'credibility', 'positivity']

# def create_complete_response(row):
#     """Create a complete response string from row data"""
#     scores = {col: float(row[col]) for col in SCORE_COLUMNS}
#     json_str = "{\n"
#     for i, (key, value) in enumerate(scores.items()):
#         json_str += f'    "{key}": {value}'
#         if i < len(scores) - 1:
#             json_str += ","
#         json_str += "\n"
#     json_str += "}"
#     return json_str

# Function to create the message prompts for tokenization
def get_message_prompts_tokenized(tokenizer, headline, content, row=None):
    """
    Create tokenized prompts with complete expected outputs
    """
    # Don't include any assistant response to start
    messages = [
        [
            SYSTEM_PROMPT,
            {"role": "user", "content": f"Title: {headline}\nContent: {content}..."}
        ]
    ]

    return tokenizer.apply_chat_template(
        messages, tokenize=False, continue_final_message=False
    )

print(complete_data.iloc[:1])

import pandas as pd
from sklearn.model_selection import train_test_split
from torch.utils.data import Dataset, DataLoader, random_split
import torch

# Load the dataset
complete_data = load_processed_dataset("/content/drive/MyDrive/AFiles/processed_data.csv")

# Prepare preprocessing function
def preprocess(df, tokenizer):
    # Apply the function to each row individually, passing the row for complete responses
    df["prompt"] = df.apply(
        lambda row: get_message_prompts_tokenized(
            tokenizer, row["title"], row["content"], row
        ),
        axis=1
    )
    # Format the labels
    df["labels"] = df.apply(
        lambda row: {
            "scale": float(row["scale"]),
            "impact": float(row["impact"]),
            "potential": float(row["potential"]),
            "legacy": float(row["legacy"]),
            "novelty": float(row["novelty"]),
            "credibility": float(row["credibility"]),
            "positivity": float(row["positivity"])
        },
        axis=1
    )
    return df


# # First split: 70% train, 30% temp
# df_train, df_temp = train_test_split(
#     complete_data,
#     test_size=0.3,
#     random_state=42,
#     stratify=complete_data['category']
# )

# # Second split of temp: 50% validation, 50% test
# df_val, df_test = train_test_split(
#     df_temp,
#     test_size=0.5,
#     random_state=42
#     # No stratification here
# )


# # Preprocess the datasets
# df_train = preprocess(df_train, tokenizer)
# df_val = preprocess(df_val, tokenizer)
# df_test = preprocess(df_test, tokenizer)

# # Create datasets
# train_dataset = NewsScoreDataset(df_train, tokenizer)
# val_dataset = NewsScoreDataset(df_val, tokenizer)
# test_dataset = NewsScoreDataset(df_test, tokenizer)

# # Create DataLoaders
# batch_size = 2
# train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
# val_dataloader = DataLoader(val_dataset, batch_size=batch_size)
# test_dataloader = DataLoader(test_dataset, batch_size=batch_size)

# # Print dataset sizes
# print(f"Training samples: {len(train_dataset)} ({len(train_dataset)/len(complete_data):.1%})")
# print(f"Validation samples: {len(val_dataset)} ({len(val_dataset)/len(complete_data):.1%})")
# print(f"Test samples: {len(test_dataset)} ({len(test_dataset)/len(complete_data):.1%})")

"""START HERE"""

# 1. Load your dataset once at the beginning
complete_data = load_processed_dataset("/content/drive/MyDrive/AFiles/processed_data.csv")
print(f"Complete dataset size: {len(complete_data)}")

# 2. Create a simple train/val/test split
df_train, df_temp = train_test_split(
    complete_data,
    test_size=0.3,
    random_state=42
)

df_val, df_test = train_test_split(
    df_temp,
    test_size=0.5,
    random_state=42
)

# 3. Simple preprocessing - just add prompts and format labels
def preprocess(df, tokenizer):
    # Generate prompts
    df["prompt"] = df.apply(
        lambda row: get_message_prompts_tokenized(tokenizer, row["title"], row["content"]),
        axis=1
    )

    # Format labels as dictionary
    df["labels"] = df.apply(
        lambda row: {
            "scale": float(row["scale"]),
            "impact": float(row["impact"]),
            "potential": float(row["potential"]),
            "legacy": float(row["legacy"]),
            "novelty": float(row["novelty"]),
            "credibility": float(row["credibility"]),
            "positivity": float(row["positivity"])
        },
        axis=1
    )
    return df

# 4. Apply preprocessing
df_train = preprocess(df_train, tokenizer)
df_val = preprocess(df_val, tokenizer)
df_test = preprocess(df_test, tokenizer)

# 5. Create datasets
train_dataset = NewsScoreDataset(df_train, tokenizer)
val_dataset = NewsScoreDataset(df_val, tokenizer)
test_dataset = NewsScoreDataset(df_test, tokenizer)

# 6. Create dataloaders
batch_size = 2
train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_dataloader = DataLoader(val_dataset, batch_size=batch_size)
test_dataloader = DataLoader(test_dataset, batch_size=batch_size)

# 7. Check sizes at the end
print(f"Training samples: {len(train_dataset)}")
print(f"Validation samples: {len(val_dataset)}")
print(f"Test samples: {len(test_dataset)}")

t i

print(df_train)

from torch.utils.data import Dataset, DataLoader, random_split
import torch

class NewsScoreDataset(Dataset):
    def __init__(self, df, tokenizer):
        self.tokenizer = tokenizer
        self.prompts = df["prompt"].tolist()
        self.labels = df["labels"].tolist()

    def __len__(self):
        return len(self.prompts)

    def __getitem__(self, idx):
        # Tokenize the prompts and prepare them for training
        prompt = self.prompts[idx]
        if not isinstance(prompt, str):
            prompt = str(prompt)

        # Format the target as a string (JSON object)
        label_dict = self.labels[idx]

        target_str = "{\n"
        # Check if label_dict is already a dictionary
        if not isinstance(label_dict, dict):
            # Try to convert to dict if it's a string representation
            try:
                import json
                label_dict = json.loads(label_dict)
            except:
                print(f"Warning: Could not parse label_dict at index {idx}, type: {type(label_dict)}")
                label_dict = {}  # Fallback to empty dict

        for i, (key, value) in enumerate(label_dict.items()):
            target_str += f'    "{key}": {value}'
            if i < len(label_dict) - 1:
                target_str += ","
            target_str += "\n"
        target_str += "}"

        # Add the EOS token
        target_str += self.tokenizer.eos_token

        # Return prompt and target
        return {
            "prompt": prompt,
            "target": target_str,
            "labels": label_dict
        }

# # Create full train dataset
# full_train_dataset = NewsScoreDataset(df_train, tokenizer)

# # Calculate sizes for train and validation splits
# # Common practice is 80-90% training, 10-20% validation
# train_size = int(0.85 * len(full_train_dataset))
# val_size = len(full_train_dataset) - train_size

# # Split the dataset
# train_dataset, val_dataset = random_split(
#     full_train_dataset,
#     [train_size, val_size],
#     generator=torch.Generator().manual_seed(42)  # Set seed for reproducibility
# )

# # Create test dataset
# test_dataset = NewsScoreDataset(df_test, tokenizer)

# # Create DataLoaders
# batch_size = 2
# train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
# val_dataloader = DataLoader(val_dataset, batch_size=batch_size)
# test_dataloader = DataLoader(test_dataset, batch_size=batch_size)

# print(f"Training samples: {len(train_dataset)}")
# print(f"Validation samples: {len(val_dataset)}")
# print(f"Test samples: {len(test_dataset)}")

# complete_data = load_processed_dataset("/content/drive/MyDrive/AFiles/processed_data.csv")

# # Split once and use these splits consistently
# df_train, df_temp = train_test_split(
#     complete_data,
#     test_size=0.3,
#     random_state=42,
#     stratify=complete_data['category'] if 'category' in complete_data.columns else None
# )

# df_val, df_test = train_test_split(
#     df_temp,
#     test_size=0.5,
#     random_state=42
# )

# # Then create your datasets from these splits
# train_dataset = NewsScoreDataset(df_train, tokenizer)
# val_dataset = NewsScoreDataset(df_val, tokenizer)
# test_dataset = NewsScoreDataset(df_test, tokenizer)



category_counts = complete_data['category'].value_counts()

# Find categories with only one sample
single_sample_categories = category_counts[category_counts == 1]

print("Categories with only one sample:")
print(single_sample_categories)

# If you want to see the actual samples:
for cat in single_sample_categories.index:
    sample = complete_data[complete_data['category'] == cat]
    print(f"\nCategory: {cat}")
    print(sample[['title', 'url']].iloc[0])  # Adjust columns as needed

# Print all category counts
print("All category counts:")
print(category_counts)

# Check if there are any extremely small categories
small_categories = category_counts[category_counts < 3]
print("\nCategories with fewer than 3 samples:")
print(small_categories)

# Check the total number of categories
print(f"\nTotal number of unique categories: {len(category_counts)}")

# Check if your stratification variable might not be 'category'
if 'category' in complete_data.columns:
    print("\nUsing 'category' for stratification")
else:
    print("\nWarning: 'category' column not found! Check what you're using for stratification")
    print("Available columns:")
    print(complete_data.columns.tolist())

stratify_by = complete_data['category'] if 'category' in complete_data.columns else None
print(f"\nStratifying by: {type(stratify_by)}")
if stratify_by is not None:
    print(f"Number of unique values in stratification column: {stratify_by.nunique()}")

    # Check the most problematic case - a category with 2 samples
    potential_issues = category_counts[category_counts == 2]
    if not potential_issues.empty:
        print("\nPotential issues - categories with exactly 2 samples:")
        print(potential_issues)

print(f"Training samples: {len(df_train)} ({len(df_train)/len(complete_data):.1%})")
print(f"Validation samples: {len(df_val)} ({len(df_val)/len(complete_data):.1%})")
print(f"Test samples: {len(df_test)} ({len(df_test)/len(complete_data):.1%})")


print(df_test.head)

"""### Defining prompts that are passed into the system"""

# complete_data.iloc[1]
len(get_message_prompts_tokenized(tokenizer, complete_data.iloc[1]["title"], complete_data.iloc[1]["content"]))

"""### Tokenise the data"""

# def preprocess(df, tokenizer):
#     # Apply the function to each row individually
#     df["prompt"] = df.apply(
#         lambda row: get_message_prompts_tokenized(tokenizer, row["title"], row["content"]),
#         axis=1
#     )
#     # Format the labels
#     df["labels"] = df.apply(
#         lambda row: {
#             "scale": float(row["scale"]),
#             "impact": float(row["impact"]),
#             "potential": float(row["potential"]),
#             "legacy": float(row["legacy"]),
#             "novelty": float(row["novelty"]),
#             "credibility": float(row["credibility"]),
#             "positivity": float(row["positivity"])
#         },
#         axis=1
#     )
#     return df

# preprocess(complete_data, tokenizer)
# print(complete_data)

train_size = 0.8
train_len = int(train_size * len(complete_data))
df_train = complete_data[:train_len]
df_test = complete_data[train_len:]
print(len(df_train), len(df_test))

def get_dataset(tokenizer, train_size=None, test_size=None):
    train_size = train_size or len(df_train)
    test_size = test_size or len(df_test)

    train_size = min(len(df_train), train_size)
    test_size = min(len(df_test), test_size)

    train_dataset = preprocess(df_train[:train_size], tokenizer)
    test_dataset = preprocess(df_test[:test_size], tokenizer)

    return train_dataset, test_dataset

example_prompt = df_train['prompt'].iloc[0]
print("Example prompt:", example_prompt)

"""[link text](https://)

### Training hyperparameters
"""



"""## Prepare Optimiser and Scheduler
preparing optmiser and learning rate scheduler
"""

learnings_rate = 1e-4
num_epochs = 10
batch_size = 4
gradient_accumulation_steps = 2
max_grad_norm = 1.0
weight_decay = 0.01
warmup_steps = 100

import torch.nn as nn
from transformers import get_scheduler
#prepare optimser
optimiser = torch.optim.AdamW(model.parameters(), lr=learnings_rate, weight_decay=weight_decay)

# calculate total training steps
total_steps = len(train_dataloader) * num_epochs // gradient_accumulation_steps

#set up learning rate scheduler with warm up
lr_scheduler = get_scheduler(
    "linear",
    optimiser,
    num_warmup_steps=warmup_steps,
    num_training_steps=total_steps
)

#loss function
loss_fn = nn.CrossEntropyLoss(ignore_index=100)

"""## Prepare training utilities
create functions to handle tokenisation and metrics tracking

### Add prompts, labels and target to the input
"""

## Implementing the training loop

tokenizer.pad_token = tokenizer.eos_token
tokenizer.pad_token_id = tokenizer.eos_token_id

"""###Â Model training and validation code"""

# full_train_dataset = complete_data

# # Calculate sizes for train and validation splits
# # Common practice is 80-90% training, 10-20% validation
# train_size = int(0.85 * len(full_train_dataset))
# val_size = len(full_train_dataset) - train_size

# # Split the dataset
# train_dataset, val_dataset = random_split(
#     full_train_dataset,
#     [train_size, val_size],
#     generator=torch.Generator().manual_seed(42)  # Set seed for reproducibility
# )

# # Create DataLoaders
# batch_size = 2  # or whatever batch size you're using
# train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
# val_dataloader = DataLoader(val_dataset, batch_size=batch_size)
# test_dataloader = DataLoader(test_dataset, batch_size=batch_size)

# print(f"Training samples: {len(train_dataset)} ({len(train_dataset)/len(full_train_dataset):.1%})")
# print(f"Validation samples: {len(val_dataset)} ({len(val_dataset)/len(full_train_dataset):.1%})")
# print(f"Test samples: {len(test_dataset)} ({len(test_dataset)/len(complete_data):.1%})")

print(full_train_dataset)

"""### New extract predicted scores

## new extract true scores
"""

def extract_true_scores(inputs):
    """Extract true scores from the input labels, handling tensors properly."""
    SCORE_COLUMNS = ['scale', 'impact', 'potential', 'legacy', 'novelty', 'credibility', 'positivity']

    if 'original_labels' not in inputs:
        print("Warning: No original_labels found")
        return [0.0] * len(SCORE_COLUMNS)

    try:
        labels = inputs['original_labels']

        # Handle dictionary of tensors case (most common)
        if isinstance(labels, dict):
            scores = []
            for col in SCORE_COLUMNS:
                if col not in labels:
                    scores.append(0.0)
                    continue

                value = labels[col]
                if torch.is_tensor(value):
                    # Handle batched tensors - use only the first item for validation
                    if value.dim() > 0:
                        # If tensor has multiple elements (batch), take first one
                        if value.numel() > 1:
                            value = value[0].item() if value.dim() > 1 else value[0].item()
                        else:
                            value = value.item()
                    else:
                        value = 0.0
                scores.append(float(value))
            return scores

        # Handle list case
        elif isinstance(labels, list):
            if len(labels) == 0:
                return [0.0] * len(SCORE_COLUMNS)

            # If it's a list of dictionaries
            if isinstance(labels[0], dict):
                result = []
                for label_dict in labels:
                    item_scores = []
                    for col in SCORE_COLUMNS:
                        value = label_dict.get(col, 0.0)
                        if torch.is_tensor(value):
                            # Handle batched tensors
                            if value.numel() > 1:
                                value = value[0].item() if value.dim() > 1 else value[0].item()
                            else:
                                value = value.item()
                        item_scores.append(float(value))
                    result.append(item_scores)
                return result
            # If it's a list of values
            else:
                return [float(val) for val in labels]

    except Exception as e:
        print(f"Error processing labels: {e}")
        # More detailed debugging information for tensor shapes
        if isinstance(inputs['original_labels'], dict):
            for k, v in inputs['original_labels'].items():
                print(f"  {k}: {type(v)}")
                if torch.is_tensor(v):
                    print(f"    Shape: {v.shape}")
                    print(f"    Elements: {v.numel()}")
                    print(f"    Data: {v}")
        return [0.0] * len(SCORE_COLUMNS)

import re
import json
def extract_predicted_scores(outputs):
    """
    Extract predicted scores from model outputs, with robust handling for early training.
    """
    SCORE_COLUMNS = ['scale', 'impact', 'potential', 'legacy', 'novelty', 'credibility', 'positivity']

    # Decode the model's generated text
    if hasattr(outputs, 'logits'):
        # During training
        generated_text = tokenizer.decode(
            outputs.logits.argmax(dim=-1)[0],
            skip_special_tokens=True
        )
    else:
        # During inference
        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

    print(f"\n===== MODEL OUTPUT ====== {generated_text[:500]}\n=====")
    # Print a sample of generated text for debugging (limited to avoid flooding logs)
    if len(generated_text) > 200:
        print(f"Generated text (sample): {generated_text[:200]}...")
    else:
        print(f"Generated text: {generated_text}")

    # Try multiple patterns to extract scores

    # 1. First try complete JSON pattern
    json_pattern = r'\{[^{]*"scale"[^}]*"impact"[^}]*"potential"[^}]*"legacy"[^}]*"novelty"[^}]*"credibility"[^}]*"positivity"[^}]*\}'
    match = re.search(json_pattern, generated_text)

    if match:
        try:
            score_text = match.group(0)
            # Clean up JSON
            score_text = re.sub(r'[^\x00-\x7F]+', '', score_text)
            score_text = score_text.replace("'", "\"")
            score_text = re.sub(r'(\w+)(?=\s*:)', r'"\1"', score_text)

            scores = json.loads(score_text)
            return [float(scores.get(col, 0.0)) for col in SCORE_COLUMNS]
        except Exception as e:
            print(f"JSON parsing failed: {e}")

    # 2. Try to extract individual scores if complete JSON pattern fails
    scores_dict = {}
    for col in SCORE_COLUMNS:
        # Look for patterns like "scale": 7 or scale: 7 or "scale" = 7
        patterns = [
            rf'"{col}"\s*:\s*(\d+(?:\.\d+)?)',
            rf'{col}\s*:\s*(\d+(?:\.\d+)?)',
            rf'"{col}"\s*=\s*(\d+(?:\.\d+)?)'
        ]

        for pattern in patterns:
            match = re.search(pattern, generated_text, re.IGNORECASE)
            if match:
                try:
                    scores_dict[col] = float(match.group(1))
                    break  # Found a match for this column, move to next
                except:
                    pass

    # If we found any scores, return them
    if scores_dict:
        print(f"Extracted partial scores: {scores_dict}")
        return [float(scores_dict.get(col, 0.0)) for col in SCORE_COLUMNS]

    print("No scores found in generated text")
    return [0.0] * len(SCORE_COLUMNS)

"""### new prepare inputs  """

# from tqdm import tqdm
# import os
# import torch.nn.functional as F


# learning_rate = 1e-4
# num_epochs = 5
# batch_size = 1  # Adjust based on your GPU memory
# gradient_accumulation_steps = 2
# clip_value = 1.0

# device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# model = model.to(device)

# save_base_path = "/content/drive/MyDrive/AFiles/saved_models"
# os.makedirs(save_base_path, exist_ok=True)


# # Training loop
# print(f"Starting training on {device}...")
# for epoch in range(num_epochs):
#     print(f"\n{'=' * 30}\nEpoch {epoch+1}/{num_epochs}\n{'=' * 30}")

#     # Set model to training mode
#     model.train()
#     total_loss = 0

#     # Progress bar for training
#     progress_bar = tqdm(enumerate(train_dataloader), total=len(train_dataloader), desc="Training")

#     for step, batch in progress_bar:
#         # Prepare inputs
#         inputs = prepare_inputs(batch, tokenizer, device)

#         # Forward pass
#         outputs = model(
#             input_ids=inputs["input_ids"],
#             attention_mask=inputs["attention_mask"],
#             labels=inputs["labels"]
#         )

#         # Calculate loss and normalize by gradient accumulation steps
#         loss = outputs.loss / gradient_accumulation_steps

#         # Update metrics
#         total_loss += loss.item() * gradient_accumulation_steps
#         avg_loss = total_loss / (step + 1)

#         # Backward pass
#         loss.backward()

#         # Update weights after accumulating gradients
#         if (step + 1) % gradient_accumulation_steps == 0 or step == len(train_dataloader) - 1:
#             # Clip gradients
#             torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)

#             # Update weights
#             optimiser.step()
#             optimiser.zero_grad()

#         # Update progress bar
#         progress_bar.set_postfix({"loss": avg_loss})

#     # Save model after each epoch
#     model_save_path = os.path.join(save_base_path, f"epoch_{epoch+1}")
#     os.makedirs(model_save_path, exist_ok=True)
#     model.save_pretrained(model_save_path)
#     tokenizer.save_pretrained(model_save_path)
#     print(f"Model saved to {model_save_path}")

# print("Training complete!")

# For input prompts (analyze distribution)
input_token_counts = []
for prompt in complete_data["prompt"].tolist():
    tokens = tokenizer.encode(str(prompt))
    input_token_counts.append(len(tokens))

# For target outputs (analyze distribution)
target_token_counts = []
for idx in range(len(complete_data)):
    label_dict = complete_data.iloc[idx]["labels"]
    target_str = "{\n"
    for i, (key, value) in enumerate(label_dict.items()):
        target_str += f'    "{key}": {value}'
        if i < len(label_dict) - 1:
            target_str += ","
        target_str += "\n"
    target_str += "}" + tokenizer.eos_token

    tokens = tokenizer.encode(target_str)
    target_token_counts.append(len(tokens))

# Plot histograms
import matplotlib.pyplot as plt
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.hist(input_token_counts, bins=30)
plt.title('Input Token Count Distribution')
plt.xlabel('Token Count')
plt.ylabel('Frequency')
plt.axvline(x=np.percentile(input_token_counts, 95), color='r', linestyle='--',
           label=f'95th percentile: {np.percentile(input_token_counts, 95):.0f}')
plt.legend()

plt.subplot(1, 2, 2)
plt.hist(target_token_counts, bins=30)
plt.title('Target Token Count Distribution')
plt.xlabel('Token Count')
plt.ylabel('Frequency')
plt.axvline(x=np.percentile(target_token_counts, 95), color='r', linestyle='--',
           label=f'95th percentile: {np.percentile(target_token_counts, 95):.0f}')
plt.legend()

plt.tight_layout()
plt.show()

print(f"Input tokens - Mean: {np.mean(input_token_counts):.1f}, Max: {max(input_token_counts)}, 95th %ile: {np.percentile(input_token_counts, 95):.1f}")
print(f"Target tokens - Mean: {np.mean(target_token_counts):.1f}, Max: {max(target_token_counts)}, 95th %ile: {np.percentile(target_token_counts, 95):.1f}")

print(f"Input shape: {inputs['input_ids'].shape}")
print(f"Target shape: {inputs['labels'].shape}")
print(f"Output logits shape: {outputs.logits.shape}")

def prepare_inputs(batch, tokenizer, device):
    """
    Prepare inputs for model training and evaluation.

    Args:
        batch: Batch of data from dataloader
        tokenizer: Tokenizer for encoding inputs
        device: Device to move tensors to

    Returns:
        Dictionary with input_ids, attention_mask, labels, and original_labels
    """
    # Get raw data from batch
    prompts = batch["prompt"]
    targets = batch["target"]
    label_dicts = batch["labels"]

    # Convert any non-string prompts to strings
    prompts = [str(p) for p in prompts]
    targets = [str(t) for t in targets]

    # Tokenize prompts for model input
    input_encodings = tokenizer(
        prompts,
        padding="max_length",
        max_length=256,  # Adjust based on your data
        return_tensors="pt",
        truncation=True
    ).to(device)

    # Tokenize targets for loss calculation
    target_encodings = tokenizer(
        targets,
        padding="max_length",
        max_length=256,  # Adjust based on your target length
        return_tensors="pt",
        truncation=True
    ).to(device)

    # Create labels tensor for model training
    labels = target_encodings.input_ids.clone()

    # Set padding tokens to -100 to ignore in loss calculation
    labels[labels == tokenizer.pad_token_id] = -100

    return {
        "input_ids": input_encodings.input_ids,
        "attention_mask": input_encodings.attention_mask,
        "labels": labels,
        "original_labels": label_dicts  # Keep original labels for metrics
    }

#print labels from top 5 samples
print(df_train[:1]["labels"])

print(val_dataset)

sample_batch = next(iter(train_dataloader))
print("Sample batch structure:", sample_batch.keys())
sample_inputs = prepare_inputs(sample_batch, tokenizer, device)
print("Prepared inputs:", sample_inputs.keys())
print("Sample labels format:", type(sample_inputs["original_labels"]))

from tqdm import tqdm
import os
import torch
import numpy as np
from sklearn.metrics import mean_squared_error, mean_absolute_error
import matplotlib.pyplot as plt
import time

# Hyperparameters
learning_rate = 1e-4
num_epochs = 5
batch_size = 2  # Adjust based on your GPU memory
gradient_accumulation_steps = 2
clip_value = 1.0
early_stopping_patience = 3
warmup_steps =

# Setup optimizer and learning rate scheduler
optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)
scheduler = torch.optim.lr_scheduler.OneCycleLR(
    optimizer,
    max_lr=learning_rate,
    steps_per_epoch=len(train_dataloader) // gradient_accumulation_steps,
    epochs=num_epochs
)

# Make sure tokenizer has padding token set
tokenizer.pad_token = tokenizer.eos_token
tokenizer.pad_token_id = tokenizer.eos_token_id

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)

# Create directories for saving
save_base_path = "/content/drive/MyDrive/AFiles/saved_models"
logs_path = "/content/drive/MyDrive/AFiles/training_logs"
os.makedirs(save_base_path, exist_ok=True)
os.makedirs(logs_path, exist_ok=True)

# For logging
training_stats = {
    'epoch': [],
    'train_loss': [],
    'val_loss': [],
    'val_mse': [],
    'val_mae': [],
    'learning_rate': [],
    'time_elapsed': []
}

best_val_loss = float('inf')
patience_counter = 0
start_time = time.time()

# Training loop
print(f"Starting training on {device}...")
for epoch in range(num_epochs):
    print(f"\n{'=' * 30}\nEpoch {epoch+1}/{num_epochs}\n{'=' * 30}")

    # Training phase
    model.train()
    total_loss = 0

    # Progress bar for training
    progress_bar = tqdm(enumerate(train_dataloader), total=len(train_dataloader), desc="Training")

    for step, batch in progress_bar:
        # Prepare inputs
        inputs = prepare_inputs(batch, tokenizer, device)

        # Forward pass
        outputs = model(
            input_ids=inputs["input_ids"],
            attention_mask=inputs["attention_mask"],
            labels=inputs["labels"]
        )

        # Calculate loss and normalize by gradient accumulation steps
        loss = outputs.loss / gradient_accumulation_steps

        # Update metrics
        total_loss += loss.item() * gradient_accumulation_steps
        avg_loss = total_loss / (step + 1)

        # Backward pass
        loss.backward()

        # Update weights after accumulating gradients
        if (step + 1) % gradient_accumulation_steps == 0 or step == len(train_dataloader) - 1:
            # Clip gradients
            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)

            # Update weights
            optimizer.step()
            scheduler.step()
            optimizer.zero_grad()

        # Update progress bar
        progress_bar.set_postfix({
            "loss": avg_loss,
            "lr": scheduler.get_last_lr()[0]
        })

    # Calculate average training loss for this epoch
    train_loss = total_loss / len(train_dataloader)

    # Validation phase
    model.eval()
    val_loss = 0
    all_preds = []
    all_labels = []

    print("\nRunning validation...")
    with torch.no_grad():
        for batch in tqdm(val_dataloader, desc="Validation"):
            inputs = prepare_inputs(batch, tokenizer, device)

            outputs = model(
                input_ids=inputs["input_ids"],
                attention_mask=inputs["attention_mask"],
                labels=inputs["labels"]
            )

            val_loss += outputs.loss.item()

            # Extract predictions and labels for metrics calculation
            predicted_scores = extract_predicted_scores(outputs)
            true_scores = extract_true_scores(inputs)

            if isinstance(true_scores[0], list):
                # Handle batch of scores
                all_labels.extend(true_scores)
                all_preds.extend([predicted_scores] * len(true_scores))
            else:
                # Handle single score
                all_labels.append(true_scores)
                all_preds.append(predicted_scores)

    # Calculate validation metrics
    val_loss = val_loss / len(val_dataloader)

    # Make sure predictions and labels are flattened correctly
    all_labels_flat = [val for sublist in all_labels for val in (sublist if isinstance(sublist, list) else [sublist])]
    all_preds_flat = [val for sublist in all_preds for val in (sublist if isinstance(sublist, list) else [sublist])]

    # Calculate metrics
    val_mse = mean_squared_error(all_labels_flat, all_preds_flat)
    val_mae = mean_absolute_error(all_labels_flat, all_preds_flat)

    # Log statistics
    time_elapsed = (time.time() - start_time) / 60.0  # minutes
    training_stats['epoch'].append(epoch + 1)
    training_stats['train_loss'].append(train_loss)
    training_stats['val_loss'].append(val_loss)
    training_stats['val_mse'].append(val_mse)
    training_stats['val_mae'].append(val_mae)
    training_stats['learning_rate'].append(scheduler.get_last_lr()[0])
    training_stats['time_elapsed'].append(time_elapsed)

    print(f"Epoch {epoch+1} - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val MSE: {val_mse:.4f}, Val MAE: {val_mae:.4f}")

    # Save model after each epoch
    model_save_path = os.path.join(save_base_path, f"epoch_{epoch+1}")
    os.makedirs(model_save_path, exist_ok=True)
    model.save_pretrained(model_save_path)
    tokenizer.save_pretrained(model_save_path)
    print(f"Model saved to {model_save_path}")

    # Check for early stopping
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        patience_counter = 0
        # Save best model
        best_model_path = os.path.join(save_base_path, "best_model")
        os.makedirs(best_model_path, exist_ok=True)
        model.save_pretrained(best_model_path)
        tokenizer.save_pretrained(best_model_path)
        print(f"New best model saved to {best_model_path}")
    else:
        patience_counter += 1
        if patience_counter >= early_stopping_patience:
            print(f"Early stopping triggered after {epoch+1} epochs")
            break

# Save final model
final_model_path = os.path.join(save_base_path, "final_model")
os.makedirs(final_model_path, exist_ok=True)
model.save_pretrained(final_model_path)
tokenizer.save_pretrained(final_model_path)
print(f"Final model saved to {final_model_path}")

# Save training stats
import pandas as pd
pd.DataFrame(training_stats).to_csv(os.path.join(logs_path, "training_stats.csv"), index=False)

# Plot training curves
plt.figure(figsize=(12, 8))
plt.subplot(2, 2, 1)
plt.plot(training_stats['epoch'], training_stats['train_loss'], label='Train Loss')
plt.plot(training_stats['epoch'], training_stats['val_loss'], label='Val Loss')
plt.title('Loss vs. Epoch')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.subplot(2, 2, 2)
plt.plot(training_stats['epoch'], training_stats['val_mse'], label='MSE')
plt.plot(training_stats['epoch'], training_stats['val_mae'], label='MAE')
plt.title('Validation Metrics vs. Epoch')
plt.xlabel('Epoch')
plt.ylabel('Metric Value')
plt.legend()

plt.subplot(2, 2, 3)
plt.plot(training_stats['epoch'], training_stats['learning_rate'])
plt.title('Learning Rate vs. Epoch')
plt.xlabel('Epoch')
plt.ylabel('Learning Rate')

plt.tight_layout()
plt.savefig(os.path.join(logs_path, "training_curves.png"))
plt.show()

print("Training complete!")

print("Validation set size:", len(val_dataset))
for i in range(min(5, len(val_dataset))):
    print(f"Example {i}:", val_dataset[i]['prompt'][:100], "...")  # Print first 100 chars

for batch_idx, batch in enumerate(val_dataloader):
    if batch_idx < 2:  # Just check first couple of batches
        print(f"Batch {batch_idx}, first prompt: {batch['prompt'][0][:50]}...")

for i in range(min(3, len(val_dataset))):
    print(f"FULL Example {i}:")
    print(val_dataset[i]['prompt'])
    print("-" * 50)



"""### saving Lora weights only using .bin format"""

# Export just the adapter weights
model_path = "/content/drive/MyDrive/AFiles/saved_models/best_model"

# Create a directory for the adapter weights
import os
lora_export_path = "/content/drive/MyDrive/AFiles/lora_weights_only"
os.makedirs(lora_export_path, exist_ok=True)

# Copy the adapter files
!cp -r {model_path}/adapter_config.json {lora_export_path}/
!cp -r {model_path}/adapter_model.safetensors {lora_export_path}/
!cp -r {model_path}/special_tokens_map.json {lora_export_path}/
!cp -r {model_path}/tokenizer.json {lora_export_path}/
!cp -r {model_path}/tokenizer_config.json {lora_export_path}/

print(f"LoRA adapter weights saved to {lora_export_path}/")

from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel, PeftConfig
import torch
import gc
# gc.collect()
# torch.cuda.empty_cache()

from peft import PeftConfig

# Use the full path to your local adapter config
config_path = "/content/drive/MyDrive/AFiles/lora_weights_only"
config = PeftConfig.from_pretrained(config_path)

print(f"Base model: {config.base_model_name_or_path}")
print(f"Task type: {config.task_type}")
print(f"Target modules: {config.target_modules}")

!pip install -q -U bitsandbytes==0.45.3
!pip install -q -U git+https://github.com/huggingface/transformers.git
!pip install -q -U git+https://github.com/huggingface/peft.git
!pip install -q -U git+https://github.com/huggingface/accelerate.git

from peft import PeftConfig

# Use the full path to your local adapter config
config_path = "/content/drive/MyDrive/AFiles/lora_weights_only"
config = PeftConfig.from_pretrained(config_path)

print(f"Base model: {config.base_model_name_or_path}")
print(f"Task type: {config.task_type}")
print(f"Target modules: {config.target_modules}")

from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import PeftModel
import torch
import gc

gc.collect()
torch.cuda.empty_cache()

# Load tokenizer from base model instead
base_model_id = config.base_model_name_or_path  # Use the base model ID from the config
tokenizer = AutoTokenizer.from_pretrained(base_model_id)
tokenizer.pad_token = tokenizer.eos_token

# Set up quantization configuration
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4"
)

# Try to load the base model with quantization
try:
    base_model = AutoModelForCausalLM.from_pretrained(
        base_model_id,
        quantization_config=quantization_config,
        device_map="auto"
    )
    print("Successfully loaded base model with 4-bit quantization")
except Exception as e:
    print(f"4-bit loading failed: {e}")

    # Fall back to 8-bit if 4-bit fails
    try:
        quantization_config = BitsAndBytesConfig(load_in_8bit=True)
        base_model = AutoModelForCausalLM.from_pretrained(
            base_model_id,
            quantization_config=quantization_config,
            device_map="auto"
        )
        print("Successfully loaded base model with 8-bit quantization")
    except Exception as e:
        print(f"8-bit loading failed: {e}")

        # Fall back to CPU as last resort
        print("Falling back to CPU...")
        base_model = AutoModelForCausalLM.from_pretrained(
            base_model_id,
            device_map="cpu"
        )
        print("Successfully loaded base model on CPU")

# Load the LoRA adapter using the full path
model = PeftModel.from_pretrained(base_model, config_path)
model.eval()

print("Model loaded successfully!")

print(f"CUDA available: {torch.cuda.is_available()}")
print(f"Current device: {torch.cuda.current_device()}")
print(f"Device name: {torch.cuda.get_device_name()}")

tokenizer = AutoTokenizer.from_pretrained(base_model_id)
tokenizer.pad_token = tokenizer.eos_token

import json
import re

def generate_scores(article_title, article_content):
    """
    Generate scores for an article using the loaded model.

    Args:
        article_title: Title of the article
        article_content: Content of the article

    Returns:
        scores: Dictionary of scores or error message
        generated_text: The model's full output text
    """
    device = next(model.parameters()).device  # Get device model is on

    # Create prompt
    prompt = get_message_prompts_tokenized(tokenizer, article_title, article_content)

    # Tokenize
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=1024).to(device)

    # Generate text
    with torch.no_grad():
        outputs = model.generate(
            input_ids=inputs["input_ids"],
            attention_mask=inputs["attention_mask"],
            max_new_tokens=500,  # Increase to allow for reasoning
            temperature=0.2,     # Lower temperature for more deterministic responses
            do_sample=True,
            top_p=0.9,
            pad_token_id=tokenizer.eos_token_id
        )


    # Decode the generated text
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    print(generated_text)
    # Try to extract the JSON part
    try:
        json_match = re.search(r'\{.*\}', generated_text, re.DOTALL)
        if json_match:
            json_str = json_match.group(0)
            scores = json.loads(json_str)
            return scores, generated_text
        else:
            return {"error": "No JSON found in generated text"}, generated_text
    except Exception as e:
        return {"error": str(e)}, generated_text

# Example usage:
scores, full_text = generate_scores("Biden Announces New Climate Policy", "President Biden announced today a new comprehensive climate policy that aims to reduce carbon emissions by 50% by 2030.")
print("Extracted scores:")
print(json.dumps(scores, indent=2))



from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Define the model ID
model_id = "meta-llama/Llama-3.2-1B-Instruct"
token = "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"  # Your Hugging Face token

# Load the tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_id, token=token, padding_side="left")
tokenizer.pad_token = tokenizer.eos_token

# Load the base model (no LoRA)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    token=token,
    torch_dtype=torch.bfloat16,  # Use bfloat16 for efficiency if available
    device_map="auto"  # Automatically choose the best device configuration
)

# Function to generate scores using the base model
def generate_scores_base_model(article_title, article_content):
    """
    Generate scores for an article using the base model.
    """
    # Get the device the model is on
    device = next(model.parameters()).device

    # Create the prompt
    prompt = get_message_prompts_tokenized(tokenizer, article_title, article_content)

    # Tokenize
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=1024).to(device)

    # Generate text
    with torch.no_grad():
        outputs = model.generate(
            input_ids=inputs["input_ids"],
            attention_mask=inputs["attention_mask"],
            max_new_tokens=500,
            temperature=0.2,
            do_sample=True,
            top_p=0.9,
            pad_token_id=tokenizer.eos_token_id
        )

    # Decode the generated text
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

    # Try to extract the JSON part
    import re
    import json
    try:
        json_match = re.search(r'\{.*\}', generated_text, re.DOTALL)
        if json_match:
            json_str = json_match.group(0)
            scores = json.loads(json_str)
            return scores, generated_text
        else:
            return {"error": "No JSON found in generated text"}, generated_text
    except Exception as e:
        return {"error": str(e)}, generated_text

# Example usage
test_title = "Biden Announces New Climate Policy"
test_content = "President Biden announced today a new comprehensive climate policy that aims to reduce carbon emissions by 50% by 2030."

scores, full_text = generate_scores_base_model(test_title, test_content)
print("Generated text:")
print(full_text)
print("\nExtracted scores:")
print(json.dumps(scores, indent=2))

model_path = "./saved_models/epoch_5"

from peft import PeftModel, PeftConfig

config = PeftConfig.from_pretrained(model_path)

test_title = "New Climate Report Shows Acceleration of Global Warming"
test_content = """Scientists have released a new report showing that global warming is accelerating faster than previously predicted. The Intergovernmental Panel on Climate Change (IPCC) report indicates that temperatures could rise by 1.5Â°C above pre-industrial levels within the next decade unless immediate action is taken to reduce carbon emissions. The report synthesizes findings from thousands of scientific studies and represents the most comprehensive assessment of climate change to date. World leaders are expected to respond with new climate pledges at the upcoming COP28 summit."""
raw_output = generate_scores(model_path, test_title, test_content)
print(raw_output)
# print("\nGenerated significance scores:")
# for factor, score in scores.items():
#     print(f"{factor}: {score}")

# print("\nRaw model output:")
# print(raw_output)

import torch
import torch.nn.functional as F

def calculate_loss(model_path, test_df, tokenizer):
    """Calculate loss on test dataset"""
    # Load model
    model = PeftModel.from_pretrained(AutoModelForCausalLM.from_pretrained(
        "meta-llama/Llama-3.2-1B-Instruct", device_map="cpu"), model_path)
    model.eval()

    total_loss = 0.0
    num_samples = 0

    with torch.no_grad():
        for idx, row in test_df.iterrows():
            # Prepare input
            prompt = get_message_prompts_tokenized(tokenizer, row['title'], row['content'])
            inputs = tokenizer(prompt, return_tensors="pt", truncation=True)

            # Prepare target (expected output)
            target_str = "{\n"
            for i, (key, value) in enumerate(row[SCORE_COLUMNS].items()):
                target_str += f'    "{key}": {value}'
                if i < len(SCORE_COLUMNS) - 1:
                    target_str += ","
                target_str += "\n"
            target_str += "}" + tokenizer.eos_token

            target = tokenizer(target_str, return_tensors="pt")

            # Forward pass
            outputs = model(
                input_ids=inputs["input_ids"],
                attention_mask=inputs["attention_mask"],
                labels=target["input_ids"]
            )

            # Add to total loss
            total_loss += outputs.loss.item()
            num_samples += 1

    # Calculate average loss
    avg_loss = total_loss / max(1, num_samples)
    return avg_loss

print(len(complete_data))
print(len(df_test))



from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt


epoch_losses = {}

for epoch in [3]:  # Choose specific checkpoints to evaluate
    model_path = f"./saved_models/epoch_{epoch}"
    loss = calculate_loss(model_path, df_test, tokenizer)
    epoch_losses[epoch] = loss
    print(f"Epoch {epoch} loss: {loss:.4f}")

# Visualize loss comparison
plt.figure(figsize=(8, 6))
epochs = list(epoch_losses.keys())
losses = list(epoch_losses.values())
plt.bar(epochs, losses, color='skyblue')
plt.title('Loss Comparison Across Epochs')
plt.xlabel('Epoch')
plt.ylabel('Evaluation Loss')
plt.xticks(epochs)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.savefig('epoch_loss_comparison.png')
plt.show()

