{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the disk.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "device = \"cuda\"  # Apple Silicon GPU\n",
    "\n",
    "load_dotenv(\"keys.env\")\n",
    "\n",
    "# Get token from environment variables\n",
    "token = os.getenv(\"token\")\n",
    "\n",
    "# Create tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, token=token, padding_side=\"left\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# Load model with float16 instead of bfloat16\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,  # Changed from bfloat16 to float16\n",
    "    device_map=\"auto\",  # Using \"auto\" instead of direct device mapping\n",
    "    token=token\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"hello what are you? \\nI am a large language model, a computer program designed to understand and generate human-like text. I'm here to help\"}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation_pipeline = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "generation_pipeline(\"hello what are you?\", max_new_tokens=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m input_prompt \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is the capital of France?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m ]\n\u001b[0;32m----> 6\u001b[0m tokenised \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokenised[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      8\u001b[0m tokenised[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llmspace/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:820\u001b[0m, in \u001b[0;36mBatchEncoding.to\u001b[0;34m(self, device, non_blocking)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;66;03m# This check catches things like APEX blindly calling \"to\" on all inputs to a module\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;66;03m# Otherwise it passes the casts down and casts the LongTensor containing the token idxs\u001b[39;00m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;66;03m# into a HalfTensor\u001b[39;00m\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m is_torch_device(device) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m--> 820\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    821\u001b[0m         k: v\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice, non_blocking\u001b[38;5;241m=\u001b[39mnon_blocking) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m v\n\u001b[1;32m    822\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    823\u001b[0m     }\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    825\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting to cast a BatchEncoding to type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(device)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. This is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llmspace/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:821\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;66;03m# This check catches things like APEX blindly calling \"to\" on all inputs to a module\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;66;03m# Otherwise it passes the casts down and casts the LongTensor containing the token idxs\u001b[39;00m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;66;03m# into a HalfTensor\u001b[39;00m\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m is_torch_device(device) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m    820\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m--> 821\u001b[0m         k: \u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m v\n\u001b[1;32m    822\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    823\u001b[0m     }\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    825\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting to cast a BatchEncoding to type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(device)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. This is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llmspace/lib/python3.9/site-packages/torch/cuda/__init__.py:310\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m     )\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    313\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    314\u001b[0m     )\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "input_prompt = [\n",
    "  \"Hello\",\n",
    "  \"What is the capital of France?\"\n",
    "]\n",
    "\n",
    "tokenised = tokenizer(input_prompt,padding=True,return_tensors=\"pt\" ,truncation=True).to(device)\n",
    "print(tokenised[\"input_ids\"].shape)\n",
    "tokenised[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenised' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtokenised\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenised' is not defined"
     ]
    }
   ],
   "source": [
    "tokenised[\"attention_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[128000, 128006,   9125, 128007,    271,  38766,   1303,  33025,   2696,\n",
      "             25,   6790,    220,   2366,     18,    198,  15724,   2696,     25,\n",
      "            220,   1313,  13806,    220,   2366,     20,    271,   2675,    527,\n",
      "            264,   7941,  15592,   6369,   6465,    889,  21881,   1093,    264,\n",
      "          55066,     13, 128009, 128006,    882, 128007,    271,   2940,   1587,\n",
      "            279,   7160,  10205,     30, 128009, 128006,  78191, 128007,    271,\n",
      "             32,   9188,    362,   9188]])\n"
     ]
    }
   ],
   "source": [
    "prompt_template = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a smart AI chatbot who speaks like a pirate.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"where does the sun rise?\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"Aye Aye\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Set pad token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "tokenised = tokenizer.apply_chat_template(\n",
    "    prompt_template,\n",
    "    add_generation_prompt=False,\n",
    "    continue_final_message=True,\n",
    "    tokenize=True,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "print(tokenised)\n",
    "# Tokenize input\n",
    "# tokenizer_output = tokenizer(\n",
    "#     tokenizer.apply_chat_template(\n",
    "#         prompt_template,\n",
    "#         tokenize=False,\n",
    "#         continue_final_message=True\n",
    "#     ),\n",
    "#     padding=True,\n",
    "#     return_tensors=\"pt\"\n",
    "# )\n",
    "\n",
    "# input_ids = tokenizer_output[\"input_ids\"].to(device)\n",
    "# attention_mask = tokenizer_output[\"attention_mask\"].to(device)\n",
    "\n",
    "# Generate text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "out = model.generate(tokenised.to(\"mps\"), max_new_tokens=25)\n",
    "# outputs = model.generate(input_ids=input_ids,attention_mask=attention_mask, max_new_tokens=20)\n",
    "\n",
    "# # Decode and print the response\n",
    "# decoded = tokenizer.batch_decode(outputs)\n",
    "# print(decoded[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 22 Feb 2025\n",
      "\n",
      "You are a smart AI chatbot who speaks like a pirate.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "where does the sun rise?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Aye Aye Captain! Yer want ta know where the sun rises, eh? Well, matey, it's a mighty big question,\n"
     ]
    }
   ],
   "source": [
    "decoded = tokenizer.batch_decode(out)\n",
    "print(decoded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "url",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "category",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "final_score",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "impact",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "scale",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "potential",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "legacy",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "novelty",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "credibility",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "positivity",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "e5b2add3-8cf2-48ce-bbda-7892f81b16d8",
       "rows": [
        [
         "0",
         "Biden Says U.S. Forces Would Defend Taiwan If China Invaded",
         "https://www.huffpost.com/entry/biden-us-forces-defend-taiwan-against-china_n_6327ec68e4b0ed021dfe3695",
         "POLITICS",
         "6.7",
         "7",
         "8",
         "8",
         "6",
         "5",
         "7",
         "3"
        ],
        [
         "1",
         "‘Beautiful And Sad At The Same Time’: Ukrainian Cultural Festival Takes On A Deeper Meaning This Year",
         "https://www.huffpost.com/entry/ukraine-festival_n_6327b4a0e4b082746beb52c7",
         "POLITICS",
         "4.2",
         "4",
         "4",
         "5",
         "3",
         "4",
         "8",
         "5"
        ],
        [
         "2",
         "Biden Says Queen's Death Left 'Giant Hole' For Royal Family",
         "https://www.huffpost.com/entry/europe-britain-royals-biden_n_63276eabe4b046aa02406a13",
         "POLITICS",
         "4.0",
         "3",
         "4",
         "3",
         "6",
         "3",
         "7",
         "5"
        ],
        [
         "3",
         "Bill To Help Afghans Who Escaped Taliban Faces Long Odds In The Senate",
         "https://www.huffpost.com/entry/afghan-adjustment-act-congress_n_6324ad6ee4b027aa4065ebef",
         "POLITICS",
         "3.8",
         "3",
         "4",
         "4",
         "3",
         "4",
         "7",
         "5"
        ],
        [
         "4",
         "Mark Meadows Complies With Justice Dept. Subpoena: Report",
         "https://www.huffpost.com/entry/capitol-riot-investigation-mark-meadows_n_63235733e4b000d988594a5d",
         "POLITICS",
         "4.5",
         "4",
         "4",
         "5",
         "5",
         "5",
         "7",
         "2"
        ]
       ],
       "shape": {
        "columns": 11,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>category</th>\n",
       "      <th>final_score</th>\n",
       "      <th>impact</th>\n",
       "      <th>scale</th>\n",
       "      <th>potential</th>\n",
       "      <th>legacy</th>\n",
       "      <th>novelty</th>\n",
       "      <th>credibility</th>\n",
       "      <th>positivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Biden Says U.S. Forces Would Defend Taiwan If ...</td>\n",
       "      <td>https://www.huffpost.com/entry/biden-us-forces...</td>\n",
       "      <td>POLITICS</td>\n",
       "      <td>6.7</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>‘Beautiful And Sad At The Same Time’: Ukrainia...</td>\n",
       "      <td>https://www.huffpost.com/entry/ukraine-festiva...</td>\n",
       "      <td>POLITICS</td>\n",
       "      <td>4.2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Biden Says Queen's Death Left 'Giant Hole' For...</td>\n",
       "      <td>https://www.huffpost.com/entry/europe-britain-...</td>\n",
       "      <td>POLITICS</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bill To Help Afghans Who Escaped Taliban Faces...</td>\n",
       "      <td>https://www.huffpost.com/entry/afghan-adjustme...</td>\n",
       "      <td>POLITICS</td>\n",
       "      <td>3.8</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mark Meadows Complies With Justice Dept. Subpo...</td>\n",
       "      <td>https://www.huffpost.com/entry/capitol-riot-in...</td>\n",
       "      <td>POLITICS</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Biden Says U.S. Forces Would Defend Taiwan If ...   \n",
       "1  ‘Beautiful And Sad At The Same Time’: Ukrainia...   \n",
       "2  Biden Says Queen's Death Left 'Giant Hole' For...   \n",
       "3  Bill To Help Afghans Who Escaped Taliban Faces...   \n",
       "4  Mark Meadows Complies With Justice Dept. Subpo...   \n",
       "\n",
       "                                                 url  category  final_score  \\\n",
       "0  https://www.huffpost.com/entry/biden-us-forces...  POLITICS          6.7   \n",
       "1  https://www.huffpost.com/entry/ukraine-festiva...  POLITICS          4.2   \n",
       "2  https://www.huffpost.com/entry/europe-britain-...  POLITICS          4.0   \n",
       "3  https://www.huffpost.com/entry/afghan-adjustme...  POLITICS          3.8   \n",
       "4  https://www.huffpost.com/entry/capitol-riot-in...  POLITICS          4.5   \n",
       "\n",
       "   impact  scale  potential  legacy  novelty  credibility  positivity  \n",
       "0       7      8          8       6        5            7           3  \n",
       "1       4      4          5       3        4            8           5  \n",
       "2       3      4          3       6        3            7           5  \n",
       "3       3      4          4       3        4            7           5  \n",
       "4       4      4          5       5        5            7           2  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data_output_files/summary_data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/naeemujeeb/PycharmProjects/GlobalHQ/GlobalLEARN\n",
      "['fetch_article.py', 'prompt_files', 'requirements.txt', 'environment.yml', 'Dockerfile', 'dataset', 'sample_articles', '__pycache__', 'openailabeling.py', 'model1.ipynb', 'data_output_files', 'keys.env', 'main.py']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'openailabeling' from '/Users/naeemujeeb/PycharmProjects/GlobalHQ/GlobalLEARN/openailabeling.py'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from .openailabeling import NewsSignificanceCalculator\n",
    "# NSC = NewsSignificanceCalculator()\n",
    "# NSC.extract_article_content(df.iloc[0][\"url\"])\n",
    "import os\n",
    "print(os.getcwd())\n",
    "print(os.listdir())\n",
    "import openailabeling\n",
    "from openailabeling import NewsSignificanceCalculator\n",
    "import importlib\n",
    "importlib.reload(openailabeling)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removing ads and new lines\n",
      "removing ads and new lines\n",
      "removing ads and new lines\n",
      "removing ads and new lines\n",
      "removing ads and new lines\n"
     ]
    }
   ],
   "source": [
    "from openailabeling import NewsSignificanceCalculator\n",
    "NSC = NewsSignificanceCalculator()\n",
    "\n",
    "SYSTEM_PROMPT = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"\"\"Return a score of each factor for the news article I have attached at the end (0-10 for each factor):\n",
    "\n",
    "Factors and weights:\n",
    "- scale : Global population impact (10=affects all humanity, 5=regional, 2=local)\n",
    "- impact : Immediate effect strength on humanity's development\n",
    "- potential : Future influence on human civilization\n",
    "- legacy : Historical milestone for human progress\n",
    "- novelty : Uniqueness in human history\n",
    "- credibility : Source reliability\n",
    "- positivity : Positive development for humanity\n",
    "\n",
    "Scale scoring guideline:\n",
    "10: Affects all of humanity directly\n",
    "8: Major global impact\n",
    "6: Multi-region impact\n",
    "4: Regional impact\n",
    "2: Local impact\n",
    "\n",
    "Impact scoring guideline:\n",
    "10: Immediate global emergency\n",
    "8: Major global change\n",
    "6: Significant regional change\n",
    "4: Moderate regional effect\n",
    "2: Local effect\n",
    "\n",
    "Return ONLY a JSON object with scores for the following news article. Use this format:\n",
    "{\n",
    "    \"scale\": N,\n",
    "    \"impact\": N,\n",
    "    \"potential\": N,\n",
    "    \"legacy\": N,\n",
    "    \"novelty\": N,\n",
    "    \"credibility\": N,\n",
    "    \"positivity\": N\n",
    "}\n",
    "\n",
    "\n",
    "Calibration Examples:\n",
    "Global Impact (6.3/10): \"Earth surpasses 1.5°C warming limit\"\n",
    "{\n",
    "    \"scale\": 9,\n",
    "    \"impact\": 8,\n",
    "    \"potential\": 9,\n",
    "    \"legacy\": 8,\n",
    "    \"novelty\": 7,\n",
    "    \"credibility\": 9,\n",
    "    \"positivity\": 2\n",
    "}\n",
    "\n",
    "Regional Conflict (5.0/10): \"Military escalation in ongoing war\"\n",
    "{\n",
    "    \"scale\": 5,\n",
    "    \"impact\": 6,\n",
    "    \"potential\": 5,\n",
    "    \"legacy\": 5,\n",
    "    \"novelty\": 5,\n",
    "    \"credibility\": 9,\n",
    "    \"positivity\": 2\n",
    "}\n",
    "\n",
    "Local Event (2.2/10): \"Regional sports championship\"\n",
    "{\n",
    "    \"scale\": 2,\n",
    "    \"impact\": 2,\n",
    "    \"potential\": 1,\n",
    "    \"legacy\": 1,\n",
    "    \"novelty\": 2,\n",
    "    \"credibility\": 8,\n",
    "    \"positivity\": 8\n",
    "}\n",
    "\n",
    "Significance ranges:\n",
    "High (6+): Major impact on human civilization\n",
    "Medium (3-5): Regional/industry significance\n",
    "Low (1-2): Local/minor impact\n",
    "Score based on the following news article\n",
    "\"\"\"\n",
    "}\n",
    "\n",
    "# Create list of user messages for each article\n",
    "user_messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"URL: {row['url']}\\nHeadline: {row['title']}\\nContent: {NSC.extract_article_content(row['url']).get('text', '')}\"\n",
    "    }\n",
    "    for _, row in df.iloc[:5].iterrows()\n",
    "]\n",
    "\n",
    "# Create complete prompt for each article\n",
    "PROMPTS = [\n",
    "    [SYSTEM_PROMPT, user_message]\n",
    "    for user_message in user_messages\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return a score of each factor for the news article I have attached at the end (0-10 for each factor):\n",
      "\n",
      "Factors and weights:\n",
      "- scale : Global population impact (10=affects all humanity, 5=regional, 2=local)\n",
      "- impact : Immediate effect strength on humanity's development\n",
      "- potential : Future influence on human civilization\n",
      "- legacy : Historical milestone for human progress\n",
      "- novelty : Uniqueness in human history\n",
      "- credibility : Source reliability\n",
      "- positivity : Positive development for humanity\n",
      "\n",
      "Scale scoring guideline:\n",
      "10: Affects all of humanity directly\n",
      "8: Major global impact\n",
      "6: Multi-region impact\n",
      "4: Regional impact\n",
      "2: Local impact\n",
      "\n",
      "Impact scoring guideline:\n",
      "10: Immediate global emergency\n",
      "8: Major global change\n",
      "6: Significant regional change\n",
      "4: Moderate regional effect\n",
      "2: Local effect\n",
      "\n",
      "Return ONLY a JSON object with scores for the following news article. Use this format:\n",
      "{\n",
      "    \"scale\": N,\n",
      "    \"impact\": N,\n",
      "    \"potential\": N,\n",
      "    \"legacy\": N,\n",
      "    \"novelty\": N,\n",
      "    \"credibility\": N,\n",
      "    \"positivity\": N\n",
      "}\n",
      "\n",
      "\n",
      "Calibration Examples:\n",
      "Global Impact (6.3/10): \"Earth surpasses 1.5°C warming limit\"\n",
      "{\n",
      "    \"scale\": 9,\n",
      "    \"impact\": 8,\n",
      "    \"potential\": 9,\n",
      "    \"legacy\": 8,\n",
      "    \"novelty\": 7,\n",
      "    \"credibility\": 9,\n",
      "    \"positivity\": 2\n",
      "}\n",
      "\n",
      "Regional Conflict (5.0/10): \"Military escalation in ongoing war\"\n",
      "{\n",
      "    \"scale\": 5,\n",
      "    \"impact\": 6,\n",
      "    \"potential\": 5,\n",
      "    \"legacy\": 5,\n",
      "    \"novelty\": 5,\n",
      "    \"credibility\": 9,\n",
      "    \"positivity\": 2\n",
      "}\n",
      "\n",
      "Local Event (2.2/10): \"Regional sports championship\"\n",
      "{\n",
      "    \"scale\": 2,\n",
      "    \"impact\": 2,\n",
      "    \"potential\": 1,\n",
      "    \"legacy\": 1,\n",
      "    \"novelty\": 2,\n",
      "    \"credibility\": 8,\n",
      "    \"positivity\": 8\n",
      "}\n",
      "\n",
      "Significance ranges:\n",
      "High (6+): Major impact on human civilization\n",
      "Medium (3-5): Regional/industry significance\n",
      "Low (1-2): Local/minor impact\n",
      "Score based on the following news article\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(SYSTEM_PROMPT[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL: https://www.huffpost.com/entry/biden-us-forces-defend-taiwan-against-china_n_6327ec68e4b0ed021dfe3695\n",
      "Headline: Biden Says U.S. Forces Would Defend Taiwan If China Invaded\n",
      "Content:  BEIJING (AP) — President Joe Biden says U.S. forces would defend Taiwan if China tries to invade the self-ruled island claimed by Beijing as part of its territory, adding to displays of official American support for the island democracy. Biden said “yes” when asked during an interview broadcast Sunday on CBS News’s “60 Minutes” program whether “U.S. forces, U.S. men and women, would defend Taiwan in the event of a Chinese invasion.”  CBS News reported the White House said after the interview U.S. policy hasn’t changed. That policy says Washington wants to see Taiwan’s status resolved peacefully but doesn’t say whether U.S. forces might be sent in response to a Chinese attack. Tension is rising following efforts by Chinese President Xi Jinping’s government to intimidate Taiwan by firing missiles into the nearby sea and flying fighter jets nearby and visits to Taipei by political figures including U.S. House Speaker Nancy Pelosi. Taiwan’s foreign ministry on Monday expressed “sincere gratitude” to Biden for “affirming the U.S. government’s rock-solid promise of security to Taiwan.”  Taiwan will “resist authoritarian expansion and aggression” and “deepen the close security partnership” with Washington and other governments “with similar thinking” to protect regional stability, the statement said. Washington is obligated by federal law to see that Taiwan has the means to defend itself but doesn’t say whether U.S. forces would be sent. The United States has no formal relations with the island but maintains informal diplomatic ties. Taiwan and China split in 1949 after a civil war that ended with the Communist Party in control of the mainland. The two governments say they are one country but dispute which is entitled to be the national leader.  Beijing criticizes official foreign contact with Taiwan’s elected government as encouragement to make its de facto independence permanent, a step the mainland says would lead to war. Washington says it doesn’t support formal independence for Taiwan, a stance Biden repeated in the interview broadcast Sunday. “Taiwan makes their own judgments about their independence,” the president said. “We’re not encouraging their being independent.”  In May, Biden said “yes” when asked at a news conference in Tokyo whether he was willing to get involved militarily to defend Taiwan if China invaded. ___\n"
     ]
    }
   ],
   "source": [
    "print(user_messages[0][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'role': 'system', 'content': 'Return a score of each factor for the news article I have attached at the end (0-10 for each factor):\\n\\nFactors and weights:\\n- scale : Global population impact (10=affects all humanity, 5=regional, 2=local)\\n- impact : Immediate effect strength on humanity\\'s development\\n- potential : Future influence on human civilization\\n- legacy : Historical milestone for human progress\\n- novelty : Uniqueness in human history\\n- credibility : Source reliability\\n- positivity : Positive development for humanity\\n\\nScale scoring guideline:\\n10: Affects all of humanity directly\\n8: Major global impact\\n6: Multi-region impact\\n4: Regional impact\\n2: Local impact\\n\\nImpact scoring guideline:\\n10: Immediate global emergency\\n8: Major global change\\n6: Significant regional change\\n4: Moderate regional effect\\n2: Local effect\\n\\nReturn ONLY a JSON object with scores for the following news article. Use this format:\\n{\\n    \"scale\": N,\\n    \"impact\": N,\\n    \"potential\": N,\\n    \"legacy\": N,\\n    \"novelty\": N,\\n    \"credibility\": N,\\n    \"positivity\": N\\n}\\n\\n\\nCalibration Examples:\\nGlobal Impact (6.3/10): \"Earth surpasses 1.5°C warming limit\"\\n{\\n    \"scale\": 9,\\n    \"impact\": 8,\\n    \"potential\": 9,\\n    \"legacy\": 8,\\n    \"novelty\": 7,\\n    \"credibility\": 9,\\n    \"positivity\": 2\\n}\\n\\nRegional Conflict (5.0/10): \"Military escalation in ongoing war\"\\n{\\n    \"scale\": 5,\\n    \"impact\": 6,\\n    \"potential\": 5,\\n    \"legacy\": 5,\\n    \"novelty\": 5,\\n    \"credibility\": 9,\\n    \"positivity\": 2\\n}\\n\\nLocal Event (2.2/10): \"Regional sports championship\"\\n{\\n    \"scale\": 2,\\n    \"impact\": 2,\\n    \"potential\": 1,\\n    \"legacy\": 1,\\n    \"novelty\": 2,\\n    \"credibility\": 8,\\n    \"positivity\": 8\\n}\\n\\nSignificance ranges:\\nHigh (6+): Major impact on human civilization\\nMedium (3-5): Regional/industry significance\\nLow (1-2): Local/minor impact\\nScore based on the following news article\\n'}, {'role': 'user', 'content': 'URL: https://www.huffpost.com/entry/biden-us-forces-defend-taiwan-against-china_n_6327ec68e4b0ed021dfe3695\\nHeadline: Biden Says U.S. Forces Would Defend Taiwan If China Invaded\\nContent:  BEIJING (AP) — President Joe Biden says U.S. forces would defend Taiwan if China tries to invade the self-ruled island claimed by Beijing as part of its territory, adding to displays of official American support for the island democracy. Biden said “yes” when asked during an interview broadcast Sunday on CBS News’s “60 Minutes” program whether “U.S. forces, U.S. men and women, would defend Taiwan in the event of a Chinese invasion.”  CBS News reported the White House said after the interview U.S. policy hasn’t changed. That policy says Washington wants to see Taiwan’s status resolved peacefully but doesn’t say whether U.S. forces might be sent in response to a Chinese attack. Tension is rising following efforts by Chinese President Xi Jinping’s government to intimidate Taiwan by firing missiles into the nearby sea and flying fighter jets nearby and visits to Taipei by political figures including U.S. House Speaker Nancy Pelosi. Taiwan’s foreign ministry on Monday expressed “sincere gratitude” to Biden for “affirming the U.S. government’s rock-solid promise of security to Taiwan.”  Taiwan will “resist authoritarian expansion and aggression” and “deepen the close security partnership” with Washington and other governments “with similar thinking” to protect regional stability, the statement said. Washington is obligated by federal law to see that Taiwan has the means to defend itself but doesn’t say whether U.S. forces would be sent. The United States has no formal relations with the island but maintains informal diplomatic ties. Taiwan and China split in 1949 after a civil war that ended with the Communist Party in control of the mainland. The two governments say they are one country but dispute which is entitled to be the national leader.  Beijing criticizes official foreign contact with Taiwan’s elected government as encouragement to make its de facto independence permanent, a step the mainland says would lead to war. Washington says it doesn’t support formal independence for Taiwan, a stance Biden repeated in the interview broadcast Sunday. “Taiwan makes their own judgments about their independence,” the president said. “We’re not encouraging their being independent.”  In May, Biden said “yes” when asked at a news conference in Tokyo whether he was willing to get involved militarily to defend Taiwan if China invaded. ___'}], [{'role': 'system', 'content': 'Return a score of each factor for the news article I have attached at the end (0-10 for each factor):\\n\\nFactors and weights:\\n- scale : Global population impact (10=affects all humanity, 5=regional, 2=local)\\n- impact : Immediate effect strength on humanity\\'s development\\n- potential : Future influence on human civilization\\n- legacy : Historical milestone for human progress\\n- novelty : Uniqueness in human history\\n- credibility : Source reliability\\n- positivity : Positive development for humanity\\n\\nScale scoring guideline:\\n10: Affects all of humanity directly\\n8: Major global impact\\n6: Multi-region impact\\n4: Regional impact\\n2: Local impact\\n\\nImpact scoring guideline:\\n10: Immediate global emergency\\n8: Major global change\\n6: Significant regional change\\n4: Moderate regional effect\\n2: Local effect\\n\\nReturn ONLY a JSON object with scores for the following news article. Use this format:\\n{\\n    \"scale\": N,\\n    \"impact\": N,\\n    \"potential\": N,\\n    \"legacy\": N,\\n    \"novelty\": N,\\n    \"credibility\": N,\\n    \"positivity\": N\\n}\\n\\n\\nCalibration Examples:\\nGlobal Impact (6.3/10): \"Earth surpasses 1.5°C warming limit\"\\n{\\n    \"scale\": 9,\\n    \"impact\": 8,\\n    \"potential\": 9,\\n    \"legacy\": 8,\\n    \"novelty\": 7,\\n    \"credibility\": 9,\\n    \"positivity\": 2\\n}\\n\\nRegional Conflict (5.0/10): \"Military escalation in ongoing war\"\\n{\\n    \"scale\": 5,\\n    \"impact\": 6,\\n    \"potential\": 5,\\n    \"legacy\": 5,\\n    \"novelty\": 5,\\n    \"credibility\": 9,\\n    \"positivity\": 2\\n}\\n\\nLocal Event (2.2/10): \"Regional sports championship\"\\n{\\n    \"scale\": 2,\\n    \"impact\": 2,\\n    \"potential\": 1,\\n    \"legacy\": 1,\\n    \"novelty\": 2,\\n    \"credibility\": 8,\\n    \"positivity\": 8\\n}\\n\\nSignificance ranges:\\nHigh (6+): Major impact on human civilization\\nMedium (3-5): Regional/industry significance\\nLow (1-2): Local/minor impact\\nScore based on the following news article\\n'}, {'role': 'user', 'content': 'URL: https://www.huffpost.com/entry/ukraine-festival_n_6327b4a0e4b082746beb52c7\\nHeadline: ‘Beautiful And Sad At The Same Time’: Ukrainian Cultural Festival Takes On A Deeper Meaning This Year\\nContent:  SILVER SPRING, Md. — For some, it’s hearing air raid sirens while talking with relatives back in Ukraine. For others, it’s the fear that Ukraine may become an economic basket case, even if it wins the war. Or that the world may forget the country’s plight altogether. Ukrainans, their friends and those simply curious about the country that’s fought Russia to standstill gathered on a sun-splashed weekend at St. Andrew Ukrainian Orthodox Cathedral in suburban Washington for the region’s biggest annual Ukrainian cultural festival.  Amid women wearing garlands in their hair, men wearing vyshyvanka — white shirts with intricately patterned embroidery down the front — and accompanied by traditional and pop Ukrainian music, the war was never far from people’s minds. What in past years had been a chance to showcase the culture and drink some beers from the home country had taken on a different, somewhat deeper meaning. “This year is particularly beautiful and sad at the same time,” said the church pastor, the Rev. Volodymr Steliac, as he formally opened the festival Saturday. “We have seen the worst of humanity but at the same time we have seen the best of humanity.” The festival opened on Day 206 of the invasion and came amid good news for Ukraine: a surprise counteroffensive in the northeast that in a few days cleared Russians from towns and villages they had fought weeks to take earlier in the war.  But the victory came with a price. In the strategic town of Izium, mass graves were found, similar to those seen in the capital Kyiv’s suburb of Bucha when the war began. “We have seen the worst of humanity but at the same time we have seen the best of humanity.” - The Rev. Volodymr Steliac, St. Andrew Ukrainian Orthodox Cathedral Oksana Sukhina, who was staffing the booth of the U.S. Ukraine Foundation, where she is project director, said she wanted to see what the impact of the latest reports of atrocities will be on Europe.  In the past, she said, some people suspected Ukrainians have exaggerated the cruelty of the Russians with reports of horrific claims like rape as a weapon of war and the theft of household goods then shipped to Russia. “Then you see Bucha, you see now Izyum. And we have yet to see Kherson and Mariupol and other places,” Sukhina said. Sukhina, 50, immigrated to the U.S. from Kyiv in 2020. Warned by former colleagues in the State Department about the likelihood of invasion, her parents arrived in Washington only days before the war.  But as the prospect of Kyiv’s being captured receded, they went back, as did one of her daughters. They joined a 23-year-old son who left his IT job in Chicago in March to help deliver food and medical supplies in Kyiv. Sukhina said she tried to convince him to stay, but could not forbid him to leave because she had participated in pro-democracy protests in 2004 and 2014. “I’m proud of them. They are trying to do whatever they can. And this is what makes us win,” she said.  Attendees of the 2022 Washington Ukrainian Festival enjoy traditional music on the grounds of the Saint Andrew Ukrainian Orthodox Cathedral Saturday Sept. 17. Jonathan Nicholson Steliac said the church was open 24-7 when the war began. He was only getting two or three hours of sleep a night as volunteers dropped off humanitarian aid, organized it and shipped it to Ukraine. In one 48-hour period, he said, two of those young volunteers lost their fathers back in Ukraine. “The war is not there alone; the war is here as well,” Steliac said. Even though the process of gathering and sending aid has become more routine, even keeping in touch with family back home, he said, can be painful.  “What was traumatic was that they were communicating with their relatives and they said, ‘Oh, a bomb fell just a couple of neighborhoods over’ and then they tried to connect again and the phone connection doesn’t go through,” Steliac said. “You think of the worst.” Alex Naumovych, a 36-year old mortgage loan officer from Ternopil in the western part of Ukraine, said he’s had calls interrupted by the air raid sirens, although now he said the alarms are likely to be ignored. Ternopil, a city of about 250,000, has no military targets, but he said his parents still hear the sirens daily. Ukraine tracks incoming missiles and activates local warning systems based on where they could potentially land, so even residents of cities not targeted are warned.  “I’m worried that, sooner or later, one of those missiles may hit my city, like my parents’ place,” Naumovych said. Naumovych arrived in the U.S. almost 11 years ago. He had about $440 to his name then, he said, and started a landscaping business he sold years later for more than $1 million. “I’m worried that, sooner or later, one of those missiles may hit my city, like my parents’ place.” - Alex Naumovych, mortage loan officer originally from Ternopil region of Ukraine  “I love this country. Like you can come with $400 and six, seven years later you can be a millionaire,” he said. One of his big worries for Ukraine now is its economy, with so many people out of work because of the war and inflation projected to hit 30%. “The economy is in bad shape,” Naumovych said. “A lot of people left Ukraine and I’m not sure they’re coming back”  The parents of software engineer Serhiy Vorobiov, 43, are near the war’s front line in Zaporizhzhya, a Russian-speaking region that includes Europe’s largest nuclear power plant. Russia has captured the plant and Ukrainians and others fear they will use it for blackmail. But Vorobiov said his parents, only 25 miles from the front lines in the city of Zaporizhzhya, still think everything is relatively safe. He said he chats with them with about the same frequency as before the war. “Maybe because there is no very high emergency at this point. I know it’s not to my defense, but that’s how it is,” he said.  Vorobiov, his wife Oksana, 33, and their three young children, Yaromyr, Myroslava and Oksana, have lived in the U.S. since 2020. He said the war has reinforced his belief in getting rid of any Russian influence on their upbringing. “I always dreamed of it, I just didn’t want to be very tough in that. But now I have all the rights and I want to use it,” Vorobiov said. “I want them to forget [the] Russian language. We are from [the] Russian-speaking part of Ukraine, but now we are fixing it, as much as we can. Forget Russian, forget we were ever together, [forget] Soviet Union. I’m happy they don’t know what Soviet Union is.”  People receiving humanitarian aid in June in Zaporizhzhya, Ukraine, near the largest nuclear plant in Europe. SOPA Images via Getty Images But in her remarks to open the festivities, Oksana Markarova, the Ukrainian ambassador to the United States, advised focusing on the present, not the past. “Please be joyful today,” she counseled. She said Russian leader Vladimir Putin and the Russians want Ukrainians to “sit down and cry” at the devastation.  “This is what Russians want us to do. And we will not give it to them.”'}], [{'role': 'system', 'content': 'Return a score of each factor for the news article I have attached at the end (0-10 for each factor):\\n\\nFactors and weights:\\n- scale : Global population impact (10=affects all humanity, 5=regional, 2=local)\\n- impact : Immediate effect strength on humanity\\'s development\\n- potential : Future influence on human civilization\\n- legacy : Historical milestone for human progress\\n- novelty : Uniqueness in human history\\n- credibility : Source reliability\\n- positivity : Positive development for humanity\\n\\nScale scoring guideline:\\n10: Affects all of humanity directly\\n8: Major global impact\\n6: Multi-region impact\\n4: Regional impact\\n2: Local impact\\n\\nImpact scoring guideline:\\n10: Immediate global emergency\\n8: Major global change\\n6: Significant regional change\\n4: Moderate regional effect\\n2: Local effect\\n\\nReturn ONLY a JSON object with scores for the following news article. Use this format:\\n{\\n    \"scale\": N,\\n    \"impact\": N,\\n    \"potential\": N,\\n    \"legacy\": N,\\n    \"novelty\": N,\\n    \"credibility\": N,\\n    \"positivity\": N\\n}\\n\\n\\nCalibration Examples:\\nGlobal Impact (6.3/10): \"Earth surpasses 1.5°C warming limit\"\\n{\\n    \"scale\": 9,\\n    \"impact\": 8,\\n    \"potential\": 9,\\n    \"legacy\": 8,\\n    \"novelty\": 7,\\n    \"credibility\": 9,\\n    \"positivity\": 2\\n}\\n\\nRegional Conflict (5.0/10): \"Military escalation in ongoing war\"\\n{\\n    \"scale\": 5,\\n    \"impact\": 6,\\n    \"potential\": 5,\\n    \"legacy\": 5,\\n    \"novelty\": 5,\\n    \"credibility\": 9,\\n    \"positivity\": 2\\n}\\n\\nLocal Event (2.2/10): \"Regional sports championship\"\\n{\\n    \"scale\": 2,\\n    \"impact\": 2,\\n    \"potential\": 1,\\n    \"legacy\": 1,\\n    \"novelty\": 2,\\n    \"credibility\": 8,\\n    \"positivity\": 8\\n}\\n\\nSignificance ranges:\\nHigh (6+): Major impact on human civilization\\nMedium (3-5): Regional/industry significance\\nLow (1-2): Local/minor impact\\nScore based on the following news article\\n'}, {'role': 'user', 'content': \"URL: https://www.huffpost.com/entry/europe-britain-royals-biden_n_63276eabe4b046aa02406a13\\nHeadline: Biden Says Queen's Death Left 'Giant Hole' For Royal Family\\nContent:  LONDON (AP) — U.S. President Joe Biden, in London for the funeral of Queen Elizabeth II, said Sunday his heart went out to the royal family, adding the queen’s death left a “giant hole.” “It’s a loss that leaves a giant hole and sometimes you think you’ll never overcome it,” said Biden, who often talks in very personal terms about loss following the death of his first wife and young daughter and later, his adult son.  The president and first lady Jill Biden have kept a low profile since landing in the United Kingdom. They traveled to Westminster Hall to pay their respects along with hundreds of thousands of mourners who waited more than 14 hours to walk past the queen’s casket. Biden and the first lady then went to Lancaster House to sign the condolence book before heading to Buckingham Palace for a reception hosted by King Charles III. The president spoke briefly after he signed the condolence book. “As I’ve told the king, she’s going to be with him every step of the way,” Biden said. “You were fortunate to have had her for 70 years. We all were.”  He said her reign was about “treating people with dignity.” “I talk about how my mother and father thought that everyone, no matter who they were … no matter where they’re from, deserved to be treated with dignity,” the president said. “And that’s exactly what she communicated. Just the way she walked by her staff, just the way that she acted.” Biden wrote in the book that “Queen Elizabeth was admired around the world for her unwavering commitment to service.” The first lady signed a condolence book for spouses and ambassadors, writing “Queen Elizabeth lived her life for the people. She served with wisdom and grace. We will never forget her warmth, kindness and the conversations we shared.”  The president also said Elizabeth modeled a sense of service. “There’s something within our capacity to do that can make things, not just the world better, but your neighborhood better, your household better, your workplace better,” he said. “That’s what she communicated to me anyway. It was an honor to meet her, an honor to meet her.” The president will return to the U.S. on Monday and head to New York for the U.N. General Assembly, where he’s to gather with Prime Minister Liz Truss, who had just taken over as the queen died. Go Ad-Free — And Protect The Free Press The next four years will change America forever. But HuffPost won't back down when it comes to providing free and impartial journalism. For the first time, we're offering an ad-free experience to qualifying contributors who support our fearless newsroom. We hope you'll join us. You've supported HuffPost before, and we'll be honest — we could use your help again. We won't back down from our mission of providing free, fair news during this critical moment. But we can't do it without you. For the first time, we're offering an ad-free experience. to qualifying contributors who support our fearless journalism. We hope you'll join us. You've supported HuffPost before, and we'll be honest — we could use your help again. We won't back down from our mission of providing free, fair news during this critical moment. But we can't do it without you. For the first time, we're offering an ad-free experience. to qualifying contributors who support our fearless journalism. We hope you'll join us. Support HuffPost Already contributed? Log in to hide these messages.\"}], [{'role': 'system', 'content': 'Return a score of each factor for the news article I have attached at the end (0-10 for each factor):\\n\\nFactors and weights:\\n- scale : Global population impact (10=affects all humanity, 5=regional, 2=local)\\n- impact : Immediate effect strength on humanity\\'s development\\n- potential : Future influence on human civilization\\n- legacy : Historical milestone for human progress\\n- novelty : Uniqueness in human history\\n- credibility : Source reliability\\n- positivity : Positive development for humanity\\n\\nScale scoring guideline:\\n10: Affects all of humanity directly\\n8: Major global impact\\n6: Multi-region impact\\n4: Regional impact\\n2: Local impact\\n\\nImpact scoring guideline:\\n10: Immediate global emergency\\n8: Major global change\\n6: Significant regional change\\n4: Moderate regional effect\\n2: Local effect\\n\\nReturn ONLY a JSON object with scores for the following news article. Use this format:\\n{\\n    \"scale\": N,\\n    \"impact\": N,\\n    \"potential\": N,\\n    \"legacy\": N,\\n    \"novelty\": N,\\n    \"credibility\": N,\\n    \"positivity\": N\\n}\\n\\n\\nCalibration Examples:\\nGlobal Impact (6.3/10): \"Earth surpasses 1.5°C warming limit\"\\n{\\n    \"scale\": 9,\\n    \"impact\": 8,\\n    \"potential\": 9,\\n    \"legacy\": 8,\\n    \"novelty\": 7,\\n    \"credibility\": 9,\\n    \"positivity\": 2\\n}\\n\\nRegional Conflict (5.0/10): \"Military escalation in ongoing war\"\\n{\\n    \"scale\": 5,\\n    \"impact\": 6,\\n    \"potential\": 5,\\n    \"legacy\": 5,\\n    \"novelty\": 5,\\n    \"credibility\": 9,\\n    \"positivity\": 2\\n}\\n\\nLocal Event (2.2/10): \"Regional sports championship\"\\n{\\n    \"scale\": 2,\\n    \"impact\": 2,\\n    \"potential\": 1,\\n    \"legacy\": 1,\\n    \"novelty\": 2,\\n    \"credibility\": 8,\\n    \"positivity\": 8\\n}\\n\\nSignificance ranges:\\nHigh (6+): Major impact on human civilization\\nMedium (3-5): Regional/industry significance\\nLow (1-2): Local/minor impact\\nScore based on the following news article\\n'}, {'role': 'user', 'content': 'URL: https://www.huffpost.com/entry/afghan-adjustment-act-congress_n_6324ad6ee4b027aa4065ebef\\nHeadline: Bill To Help Afghans Who Escaped Taliban Faces Long Odds In The Senate\\nContent: WASHINGTON — A new bipartisan bill to give Afghans in America a pathway to permanent residency stands a slim chance of winning enough Republican support to become law in the near future. The U.S. brought in tens of thousands of people as part of its hasty retreat from Afghanistan last year but has not said that they can stay. The Afghan Adjustment Act, introduced in August by Sen. Amy Klobuchar (D-Minn.) with three Republican co-sponsors, would create a path to permanent residency for more than 77,000 Afghans who have arrived in the U.S. since last summer.  “Giving our Afghan allies – many of whom worked alongside our U.S. Military – a chance to apply for permanent legal status is the right and necessary thing to do,” Klobuchar told HuffPost in a statement. “This bipartisan legislation will provide Afghans who submit to additional vetting a green card to live and work in the United States.” The bill would also expand eligibility for so-called special immigrant visas to other at-risk groups still in Afghanistan, such as certain former members of the Afghan armed forces. “These people have no place to go,” bill co-sponsor Sen. Lindsey Graham (R-S.C.) told HuffPost. “Their country has fallen into hell.”  But Graham acknowledged that it would be difficult to win over enough Republicans to get the bill through the Senate, saying his colleagues have “a legitimate concern” that Afghan nationals could pose a security risk. Sen. Rick Scott (R-Fla.), for instance, argued that President Joe Biden’s administration failed to vet the 80,000 Afghans admitted to the U.S. last year. A Defense Department report from February said its records on 50 Afghan evacuees indicated “potentially significant security concerns.” “I think we need to start by finding out who’s here and what their background is, and making sure people around the country know where these people went to,” Scott told HuffPost.  As November’s midterm elections approach, Republicans have been more hostile toward immigrants in general, with some describing them as invaders pouring through an insecure southern border. GOP governors have even sent buses and planes full of migrants to Democratic-led cities as a political stunt. In August 2021, the Biden administration used a measure known as humanitarian parole to allow Afghans to enter the U.S. for a two-year period. Officials ran evacuees’ fingerprints and records through criminal and terrorism databases at military installations in Europe and the Middle East. Parole is not recognized as an immigration status, and it offers no road to permanent residency. The main options for Afghans are asylum and special immigrant status, but these can be complicated and costly to attain.  The Biden administration announced in March that Afghans would be eligible for temporary protected status, preventing them from having to return to unsafe conditions, but TPS does not guarantee permanent residency. Advocates of the Afghan Adjustment Act — who point to the fact that Afghans in the U.S. could lose eligibility for humanitarian parole or temporary protected status next year and potentially face deportation — are optimistic about the bill’s prospects. “Even if we run out of time this month, we have other opportunities to get this done,” said Chris Purdy, the director of Veterans for American Ideals and Outreach at the nonprofit Human Rights First. “Passing this bill is a major priority for members in both houses. And every time we’ve brought this to Congress, we’ve gotten more and more supporters. Passage of the Afghan Adjustment Act is not a matter of if; it’s a matter of when.”  There has been less violence in Afghanistan since the U.S. finished its withdrawal last summer, but the ruling Taliban reportedly still target former members of the military and government officials, despite a declared amnesty. The government is accused of persecuting minority groups and imprisoning and torturing journalists, while women face restrictive new laws. The country is also experiencing an economic crisis, which is primarily the result of frozen assets in the United States, as well as other factors such as drought. Still, the near future is unlikely to see a critical mass of support for protecting Afghan evacuees, and Republicans could take control of one or both chambers of Congress next year, which would make this even more difficult. “I just wish we could all agree, conceptually, that the people who are here have no place to go back, and they were involved in a war that we were involved in for 20 years and how we treat them will say a lot about how people will fight with us in the future,” Graham said.  On Thursday, a small number of supporters rallied for the bill outside of Congress. “Many of those we are advocating for, if not all, have made the ultimate sacrifice by putting their lives in danger protecting democracy and helping our men and women in uniform,” Jawaid Kotwal, an Afghan American, told Huffpost.'}], [{'role': 'system', 'content': 'Return a score of each factor for the news article I have attached at the end (0-10 for each factor):\\n\\nFactors and weights:\\n- scale : Global population impact (10=affects all humanity, 5=regional, 2=local)\\n- impact : Immediate effect strength on humanity\\'s development\\n- potential : Future influence on human civilization\\n- legacy : Historical milestone for human progress\\n- novelty : Uniqueness in human history\\n- credibility : Source reliability\\n- positivity : Positive development for humanity\\n\\nScale scoring guideline:\\n10: Affects all of humanity directly\\n8: Major global impact\\n6: Multi-region impact\\n4: Regional impact\\n2: Local impact\\n\\nImpact scoring guideline:\\n10: Immediate global emergency\\n8: Major global change\\n6: Significant regional change\\n4: Moderate regional effect\\n2: Local effect\\n\\nReturn ONLY a JSON object with scores for the following news article. Use this format:\\n{\\n    \"scale\": N,\\n    \"impact\": N,\\n    \"potential\": N,\\n    \"legacy\": N,\\n    \"novelty\": N,\\n    \"credibility\": N,\\n    \"positivity\": N\\n}\\n\\n\\nCalibration Examples:\\nGlobal Impact (6.3/10): \"Earth surpasses 1.5°C warming limit\"\\n{\\n    \"scale\": 9,\\n    \"impact\": 8,\\n    \"potential\": 9,\\n    \"legacy\": 8,\\n    \"novelty\": 7,\\n    \"credibility\": 9,\\n    \"positivity\": 2\\n}\\n\\nRegional Conflict (5.0/10): \"Military escalation in ongoing war\"\\n{\\n    \"scale\": 5,\\n    \"impact\": 6,\\n    \"potential\": 5,\\n    \"legacy\": 5,\\n    \"novelty\": 5,\\n    \"credibility\": 9,\\n    \"positivity\": 2\\n}\\n\\nLocal Event (2.2/10): \"Regional sports championship\"\\n{\\n    \"scale\": 2,\\n    \"impact\": 2,\\n    \"potential\": 1,\\n    \"legacy\": 1,\\n    \"novelty\": 2,\\n    \"credibility\": 8,\\n    \"positivity\": 8\\n}\\n\\nSignificance ranges:\\nHigh (6+): Major impact on human civilization\\nMedium (3-5): Regional/industry significance\\nLow (1-2): Local/minor impact\\nScore based on the following news article\\n'}, {'role': 'user', 'content': 'URL: https://www.huffpost.com/entry/capitol-riot-investigation-mark-meadows_n_63235733e4b000d988594a5d\\nHeadline: Mark Meadows Complies With Justice Dept. Subpoena: Report\\nContent:  WASHINGTON (AP) — Mark Meadows, the White House chief of staff under former President Donald Trump, has complied with a Justice Department subpoena and turned over records as part of a federal investigation into the Jan. 6, 2021 assault on the Capitol and efforts to overturn the 2020 presidential election, a person familiar the matter said Thursday. The records produced by Meadows are the same ones he earlier provided to a House committee conducting a similar investigation, according to the person, who spoke with The Associated Press on condition of anonymity to discuss an ongoing Justice Department probe.  The subpoena to Meadows, first reported by CNN, makes clear that Justice Department officials are seeking information from the most senior of Trump’s White House advisers as they examine wide-ranging efforts to overturn the results of the election won by Democrat Joe Biden. The department, whose work at times has mirrored or overlapped with that of the committee, this month served a broad wave of grand jury subpoenas and search warrants to Trump allies. Meadows has been a pivotal figure in the House investigation, his name invoked repeatedly in testimony by other Trump advisers, including by his own top aide. He had provided the committee with thousands of text messages, including communications with outside Trump allies and advisers.  In a filing in April in a federal lawsuit over his House subpoena, a lawyer for Meadows accused the committee of trying to vilify him publicly, noting that all of the texts it had been provided had been disclosed to the news media. The committee declined at the time to respond to the accusation. Meadows did not provide to the committee records he believed were subject to claims of executive privilege and those documents were also not produced to the Justice Department. ___ Follow Eric Tucker on Twitter at http://www.twitter.com/etuckerAP ___'}]]\n"
     ]
    }
   ],
   "source": [
    "print(PROMPTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "tokenised_output = tokenizer(\n",
    "  tokenizer.apply_chat_template(\n",
    "    PROMPTS,\n",
    "    tokenize=False,\n",
    "    continue_final_message=True\n",
    "  ),\n",
    "  padding=True,\n",
    "  return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "input_ids = tokenised_output[\"input_ids\"].to(device)\n",
    "attention_mask = tokenised_output[\"attention_mask\"].to(device)\n",
    "\n",
    "# tokenised = tokenizer.apply_chat_template(\n",
    "#   PROMPTS,\n",
    "#   continue_final_message=True,\n",
    "#   padding=True,\n",
    "#   return_tensors=\"pt\"\n",
    "# ).to(device)\n",
    "out = model.generate(input_ids=input_ids, attention_mask=attention_mask,  max_new_tokens=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['system\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 24 Feb 2025\\n\\nReturn a score of each factor for the news article I have attached at the end (0-10 for each factor):\\n\\nFactors and weights:\\n- scale : Global population impact (10=affects all humanity, 5=regional, 2=local)\\n- impact : Immediate effect strength on humanity\\'s development\\n- potential : Future influence on human civilization\\n- legacy : Historical milestone for human progress\\n- novelty : Uniqueness in human history\\n- credibility : Source reliability\\n- positivity : Positive development for humanity\\n\\nScale scoring guideline:\\n10: Affects all of humanity directly\\n8: Major global impact\\n6: Multi-region impact\\n4: Regional impact\\n2: Local impact\\n\\nImpact scoring guideline:\\n10: Immediate global emergency\\n8: Major global change\\n6: Significant regional change\\n4: Moderate regional effect\\n2: Local effect\\n\\nReturn ONLY a JSON object with scores for the following news article. Use this format:\\n{\\n    \"scale\": N,\\n    \"impact\": N,\\n    \"potential\": N,\\n    \"legacy\": N,\\n    \"novelty\": N,\\n    \"credibility\": N,\\n    \"positivity\": N\\n}\\n\\n\\nCalibration Examples:\\nGlobal Impact (6.3/10): \"Earth surpasses 1.5°C warming limit\"\\n{\\n    \"scale\": 9,\\n    \"impact\": 8,\\n    \"potential\": 9,\\n    \"legacy\": 8,\\n    \"novelty\": 7,\\n    \"credibility\": 9,\\n    \"positivity\": 2\\n}\\n\\nRegional Conflict (5.0/10): \"Military escalation in ongoing war\"\\n{\\n    \"scale\": 5,\\n    \"impact\": 6,\\n    \"potential\": 5,\\n    \"legacy\": 5,\\n    \"novelty\": 5,\\n    \"credibility\": 9,\\n    \"positivity\": 2\\n}\\n\\nLocal Event (2.2/10): \"Regional sports championship\"\\n{\\n    \"scale\": 2,\\n    \"impact\": 2,\\n    \"potential\": 1,\\n    \"legacy\": 1,\\n    \"novelty\": 2,\\n    \"credibility\": 8,\\n    \"positivity\": 8\\n}\\n\\nSignificance ranges:\\nHigh (6+): Major impact on human civilization\\nMedium (3-5): Regional/industry significance\\nLow (1-2): Local/minor impact\\nScore based on the following news articleuser\\n\\nURL: https://www.huffpost.com/entry/biden-us-forces-defend-taiwan-against-china_n_6327ec68e4b0ed021dfe3695\\nHeadline: Biden Says U.S. Forces Would Defend Taiwan If China Invaded\\nContent:  BEIJING (AP) — President Joe Biden says U.S. forces would defend Taiwan if China tries to invade the self-ruled island claimed by Beijing as part of its territory, adding to displays of official American support for the island democracy. Biden said “yes” when asked during an interview broadcast Sunday on CBS News’s “60 Minutes” program whether “U.S. forces, U.S. men and women, would defend Taiwan in the event of a Chinese invasion.”  CBS News reported the White House said after the interview U.S. policy hasn’t changed. That policy says Washington wants to see Taiwan’s status resolved peacefully but doesn’t say whether U.S. forces might be sent in response to a Chinese attack. Tension is rising following efforts by Chinese President Xi Jinping’s government to intimidate Taiwan by firing missiles into the nearby sea and flying fighter jets nearby and visits to Taipei by political figures including U.S. House Speaker Nancy Pelosi. Taiwan’s foreign ministry on Monday expressed “sincere gratitude” to Biden for “affirming the U.S. government’s rock-solid promise of security to Taiwan.”  Taiwan will “resist authoritarian expansion and aggression” and “deepen the close security partnership” with Washington and other governments “with similar thinking” to protect regional stability, the statement said. Washington is obligated by federal law to see that Taiwan has the means to defend itself but doesn’t say whether U.S. forces would be sent. The United States has no formal relations with the island but maintains informal diplomatic ties. Taiwan and China split in 1949 after a civil war that ended with the Communist Party in control of the mainland. The two governments say they are one country but dispute which is entitled to be the national leader.  Beijing criticizes official foreign contact with Taiwan’s elected government as encouragement to make its de facto independence permanent, a step the mainland says would lead to war. Washington says it doesn’t support formal independence for Taiwan, a stance Biden repeated in the interview broadcast Sunday. “Taiwan makes their own judgments about their independence,” the president said. “We’re not encouraging their being independent.”  In May, Biden said “yes” when asked at a news conference in Tokyo whether he was willing to get involved militarily to defend Taiwan if China invaded. ___\\nSource: AP\\n}\\n\\n{\\n    \"scale\": 6,\\n    \"impact\": 8,\\n    \"potential\": 9,\\n    \"legacy\": 8,\\n    \"novelty\": 7,\\n    \"credibility\": ', 'system\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 24 Feb 2025\\n\\nReturn a score of each factor for the news article I have attached at the end (0-10 for each factor):\\n\\nFactors and weights:\\n- scale : Global population impact (10=affects all humanity, 5=regional, 2=local)\\n- impact : Immediate effect strength on humanity\\'s development\\n- potential : Future influence on human civilization\\n- legacy : Historical milestone for human progress\\n- novelty : Uniqueness in human history\\n- credibility : Source reliability\\n- positivity : Positive development for humanity\\n\\nScale scoring guideline:\\n10: Affects all of humanity directly\\n8: Major global impact\\n6: Multi-region impact\\n4: Regional impact\\n2: Local impact\\n\\nImpact scoring guideline:\\n10: Immediate global emergency\\n8: Major global change\\n6: Significant regional change\\n4: Moderate regional effect\\n2: Local effect\\n\\nReturn ONLY a JSON object with scores for the following news article. Use this format:\\n{\\n    \"scale\": N,\\n    \"impact\": N,\\n    \"potential\": N,\\n    \"legacy\": N,\\n    \"novelty\": N,\\n    \"credibility\": N,\\n    \"positivity\": N\\n}\\n\\n\\nCalibration Examples:\\nGlobal Impact (6.3/10): \"Earth surpasses 1.5°C warming limit\"\\n{\\n    \"scale\": 9,\\n    \"impact\": 8,\\n    \"potential\": 9,\\n    \"legacy\": 8,\\n    \"novelty\": 7,\\n    \"credibility\": 9,\\n    \"positivity\": 2\\n}\\n\\nRegional Conflict (5.0/10): \"Military escalation in ongoing war\"\\n{\\n    \"scale\": 5,\\n    \"impact\": 6,\\n    \"potential\": 5,\\n    \"legacy\": 5,\\n    \"novelty\": 5,\\n    \"credibility\": 9,\\n    \"positivity\": 2\\n}\\n\\nLocal Event (2.2/10): \"Regional sports championship\"\\n{\\n    \"scale\": 2,\\n    \"impact\": 2,\\n    \"potential\": 1,\\n    \"legacy\": 1,\\n    \"novelty\": 2,\\n    \"credibility\": 8,\\n    \"positivity\": 8\\n}\\n\\nSignificance ranges:\\nHigh (6+): Major impact on human civilization\\nMedium (3-5): Regional/industry significance\\nLow (1-2): Local/minor impact\\nScore based on the following news articleuser\\n\\nURL: https://www.huffpost.com/entry/ukraine-festival_n_6327b4a0e4b082746beb52c7\\nHeadline: ‘Beautiful And Sad At The Same Time’: Ukrainian Cultural Festival Takes On A Deeper Meaning This Year\\nContent:  SILVER SPRING, Md. — For some, it’s hearing air raid sirens while talking with relatives back in Ukraine. For others, it’s the fear that Ukraine may become an economic basket case, even if it wins the war. Or that the world may forget the country’s plight altogether. Ukrainans, their friends and those simply curious about the country that’s fought Russia to standstill gathered on a sun-splashed weekend at St. Andrew Ukrainian Orthodox Cathedral in suburban Washington for the region’s biggest annual Ukrainian cultural festival.  Amid women wearing garlands in their hair, men wearing vyshyvanka — white shirts with intricately patterned embroidery down the front — and accompanied by traditional and pop Ukrainian music, the war was never far from people’s minds. What in past years had been a chance to showcase the culture and drink some beers from the home country had taken on a different, somewhat deeper meaning. “This year is particularly beautiful and sad at the same time,” said the church pastor, the Rev. Volodymr Steliac, as he formally opened the festival Saturday. “We have seen the worst of humanity but at the same time we have seen the best of humanity.” The festival opened on Day 206 of the invasion and came amid good news for Ukraine: a surprise counteroffensive in the northeast that in a few days cleared Russians from towns and villages they had fought weeks to take earlier in the war.  But the victory came with a price. In the strategic town of Izium, mass graves were found, similar to those seen in the capital Kyiv’s suburb of Bucha when the war began. “We have seen the worst of humanity but at the same time we have seen the best of humanity.” - The Rev. Volodymr Steliac, St. Andrew Ukrainian Orthodox Cathedral Oksana Sukhina, who was staffing the booth of the U.S. Ukraine Foundation, where she is project director, said she wanted to see what the impact of the latest reports of atrocities will be on Europe.  In the past, she said, some people suspected Ukrainians have exaggerated the cruelty of the Russians with reports of horrific claims like rape as a weapon of war and the theft of household goods then shipped to Russia. “Then you see Bucha, you see now Izyum. And we have yet to see Kherson and Mariupol and other places,” Sukhina said. Sukhina, 50, immigrated to the U.S. from Kyiv in 2020. Warned by former colleagues in the State Department about the likelihood of invasion, her parents arrived in Washington only days before the war.  But as the prospect of Kyiv’s being captured receded, they went back, as did one of her daughters. They joined a 23-year-old son who left his IT job in Chicago in March to help deliver food and medical supplies in Kyiv. Sukhina said she tried to convince him to stay, but could not forbid him to leave because she had participated in pro-democracy protests in 2004 and 2014. “I’m proud of them. They are trying to do whatever they can. And this is what makes us win,” she said.  Attendees of the 2022 Washington Ukrainian Festival enjoy traditional music on the grounds of the Saint Andrew Ukrainian Orthodox Cathedral Saturday Sept. 17. Jonathan Nicholson Steliac said the church was open 24-7 when the war began. He was only getting two or three hours of sleep a night as volunteers dropped off humanitarian aid, organized it and shipped it to Ukraine. In one 48-hour period, he said, two of those young volunteers lost their fathers back in Ukraine. “The war is not there alone; the war is here as well,” Steliac said. Even though the process of gathering and sending aid has become more routine, even keeping in touch with family back home, he said, can be painful.  “What was traumatic was that they were communicating with their relatives and they said, ‘Oh, a bomb fell just a couple of neighborhoods over’ and then they tried to connect again and the phone connection doesn’t go through,” Steliac said. “You think of the worst.” Alex Naumovych, a 36-year old mortgage loan officer from Ternopil in the western part of Ukraine, said he’s had calls interrupted by the air raid sirens, although now he said the alarms are likely to be ignored. Ternopil, a city of about 250,000, has no military targets, but he said his parents still hear the sirens daily. Ukraine tracks incoming missiles and activates local warning systems based on where they could potentially land, so even residents of cities not targeted are warned.  “I’m worried that, sooner or later, one of those missiles may hit my city, like my parents’ place,” Naumovych said. Naumovych arrived in the U.S. almost 11 years ago. He had about $440 to his name then, he said, and started a landscaping business he sold years later for more than $1 million. “I’m worried that, sooner or later, one of those missiles may hit my city, like my parents’ place.” - Alex Naumovych, mortage loan officer originally from Ternopil region of Ukraine  “I love this country. Like you can come with $400 and six, seven years later you can be a millionaire,” he said. One of his big worries for Ukraine now is its economy, with so many people out of work because of the war and inflation projected to hit 30%. “The economy is in bad shape,” Naumovych said. “A lot of people left Ukraine and I’m not sure they’re coming back”  The parents of software engineer Serhiy Vorobiov, 43, are near the war’s front line in Zaporizhzhya, a Russian-speaking region that includes Europe’s largest nuclear power plant. Russia has captured the plant and Ukrainians and others fear they will use it for blackmail. But Vorobiov said his parents, only 25 miles from the front lines in the city of Zaporizhzhya, still think everything is relatively safe. He said he chats with them with about the same frequency as before the war. “Maybe because there is no very high emergency at this point. I know it’s not to my defense, but that’s how it is,” he said.  Vorobiov, his wife Oksana, 33, and their three young children, Yaromyr, Myroslava and Oksana, have lived in the U.S. since 2020. He said the war has reinforced his belief in getting rid of any Russian influence on their upbringing. “I always dreamed of it, I just didn’t want to be very tough in that. But now I have all the rights and I want to use it,” Vorobiov said. “I want them to forget [the] Russian language. We are from [the] Russian-speaking part of Ukraine, but now we are fixing it, as much as we can. Forget Russian, forget we were ever together, [forget] Soviet Union. I’m happy they don’t know what Soviet Union is.”  People receiving humanitarian aid in June in Zaporizhzhya, Ukraine, near the largest nuclear plant in Europe. SOPA Images via Getty Images But in her remarks to open the festivities, Oksana Markarova, the Ukrainian ambassador to the United States, advised focusing on the present, not the past. “Please be joyful today,” she counseled. She said Russian leader Vladimir Putin and the Russians want Ukrainians to “sit down and cry” at the devastation.  “This is what Russians want us to do. And we will not give it to them.” Markarova said. “We are fighting for our country’s freedom. We are fighting for our people’s freedom. We are fighting for our future.” - Oksana Markarova, Ukrainian ambassador to the United States “We are fighting for', 'system\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 24 Feb 2025\\n\\nReturn a score of each factor for the news article I have attached at the end (0-10 for each factor):\\n\\nFactors and weights:\\n- scale : Global population impact (10=affects all humanity, 5=regional, 2=local)\\n- impact : Immediate effect strength on humanity\\'s development\\n- potential : Future influence on human civilization\\n- legacy : Historical milestone for human progress\\n- novelty : Uniqueness in human history\\n- credibility : Source reliability\\n- positivity : Positive development for humanity\\n\\nScale scoring guideline:\\n10: Affects all of humanity directly\\n8: Major global impact\\n6: Multi-region impact\\n4: Regional impact\\n2: Local impact\\n\\nImpact scoring guideline:\\n10: Immediate global emergency\\n8: Major global change\\n6: Significant regional change\\n4: Moderate regional effect\\n2: Local effect\\n\\nReturn ONLY a JSON object with scores for the following news article. Use this format:\\n{\\n    \"scale\": N,\\n    \"impact\": N,\\n    \"potential\": N,\\n    \"legacy\": N,\\n    \"novelty\": N,\\n    \"credibility\": N,\\n    \"positivity\": N\\n}\\n\\n\\nCalibration Examples:\\nGlobal Impact (6.3/10): \"Earth surpasses 1.5°C warming limit\"\\n{\\n    \"scale\": 9,\\n    \"impact\": 8,\\n    \"potential\": 9,\\n    \"legacy\": 8,\\n    \"novelty\": 7,\\n    \"credibility\": 9,\\n    \"positivity\": 2\\n}\\n\\nRegional Conflict (5.0/10): \"Military escalation in ongoing war\"\\n{\\n    \"scale\": 5,\\n    \"impact\": 6,\\n    \"potential\": 5,\\n    \"legacy\": 5,\\n    \"novelty\": 5,\\n    \"credibility\": 9,\\n    \"positivity\": 2\\n}\\n\\nLocal Event (2.2/10): \"Regional sports championship\"\\n{\\n    \"scale\": 2,\\n    \"impact\": 2,\\n    \"potential\": 1,\\n    \"legacy\": 1,\\n    \"novelty\": 2,\\n    \"credibility\": 8,\\n    \"positivity\": 8\\n}\\n\\nSignificance ranges:\\nHigh (6+): Major impact on human civilization\\nMedium (3-5): Regional/industry significance\\nLow (1-2): Local/minor impact\\nScore based on the following news articleuser\\n\\nURL: https://www.huffpost.com/entry/europe-britain-royals-biden_n_63276eabe4b046aa02406a13\\nHeadline: Biden Says Queen\\'s Death Left \\'Giant Hole\\' For Royal Family\\nContent:  LONDON (AP) — U.S. President Joe Biden, in London for the funeral of Queen Elizabeth II, said Sunday his heart went out to the royal family, adding the queen’s death left a “giant hole.” “It’s a loss that leaves a giant hole and sometimes you think you’ll never overcome it,” said Biden, who often talks in very personal terms about loss following the death of his first wife and young daughter and later, his adult son.  The president and first lady Jill Biden have kept a low profile since landing in the United Kingdom. They traveled to Westminster Hall to pay their respects along with hundreds of thousands of mourners who waited more than 14 hours to walk past the queen’s casket. Biden and the first lady then went to Lancaster House to sign the condolence book before heading to Buckingham Palace for a reception hosted by King Charles III. The president spoke briefly after he signed the condolence book. “As I’ve told the king, she’s going to be with him every step of the way,” Biden said. “You were fortunate to have had her for 70 years. We all were.”  He said her reign was about “treating people with dignity.” “I talk about how my mother and father thought that everyone, no matter who they were … no matter where they’re from, deserved to be treated with dignity,” the president said. “And that’s exactly what she communicated. Just the way she walked by her staff, just the way that she acted.” Biden wrote in the book that “Queen Elizabeth was admired around the world for her unwavering commitment to service.” The first lady signed a condolence book for spouses and ambassadors, writing “Queen Elizabeth lived her life for the people. She served with wisdom and grace. We will never forget her warmth, kindness and the conversations we shared.”  The president also said Elizabeth modeled a sense of service. “There’s something within our capacity to do that can make things, not just the world better, but your neighborhood better, your household better, your workplace better,” he said. “That’s what she communicated to me anyway. It was an honor to meet her, an honor to meet her.” The president will return to the U.S. on Monday and head to New York for the U.N. General Assembly, where he’s to gather with Prime Minister Liz Truss, who had just taken over as the queen died. Go Ad-Free — And Protect The Free Press The next four years will change America forever. But HuffPost won\\'t back down when it comes to providing free and impartial journalism. For the first time, we\\'re offering an ad-free experience to qualifying contributors who support our fearless newsroom. We hope you\\'ll join us. You\\'ve supported HuffPost before, and we\\'ll be honest — we could use your help again. We won\\'t back down from our mission of providing free, fair news during this critical moment. But we can\\'t do it without you. For the first time, we\\'re offering an ad-free experience. to qualifying contributors who support our fearless journalism. We hope you\\'ll join us. You\\'ve supported HuffPost before, and we\\'ll be honest — we could use your help again. We won\\'t back down from our mission of providing free, fair news during this critical moment. But we can\\'t do it without you. For the first time, we\\'re offering an ad-free experience. to qualifying contributors who support our fearless journalism. We hope you\\'ll join us. Support HuffPost Already contributed? Log in to hide these messages.', 'system\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 24 Feb 2025\\n\\nReturn a score of each factor for the news article I have attached at the end (0-10 for each factor):\\n\\nFactors and weights:\\n- scale : Global population impact (10=affects all humanity, 5=regional, 2=local)\\n- impact : Immediate effect strength on humanity\\'s development\\n- potential : Future influence on human civilization\\n- legacy : Historical milestone for human progress\\n- novelty : Uniqueness in human history\\n- credibility : Source reliability\\n- positivity : Positive development for humanity\\n\\nScale scoring guideline:\\n10: Affects all of humanity directly\\n8: Major global impact\\n6: Multi-region impact\\n4: Regional impact\\n2: Local impact\\n\\nImpact scoring guideline:\\n10: Immediate global emergency\\n8: Major global change\\n6: Significant regional change\\n4: Moderate regional effect\\n2: Local effect\\n\\nReturn ONLY a JSON object with scores for the following news article. Use this format:\\n{\\n    \"scale\": N,\\n    \"impact\": N,\\n    \"potential\": N,\\n    \"legacy\": N,\\n    \"novelty\": N,\\n    \"credibility\": N,\\n    \"positivity\": N\\n}\\n\\n\\nCalibration Examples:\\nGlobal Impact (6.3/10): \"Earth surpasses 1.5°C warming limit\"\\n{\\n    \"scale\": 9,\\n    \"impact\": 8,\\n    \"potential\": 9,\\n    \"legacy\": 8,\\n    \"novelty\": 7,\\n    \"credibility\": 9,\\n    \"positivity\": 2\\n}\\n\\nRegional Conflict (5.0/10): \"Military escalation in ongoing war\"\\n{\\n    \"scale\": 5,\\n    \"impact\": 6,\\n    \"potential\": 5,\\n    \"legacy\": 5,\\n    \"novelty\": 5,\\n    \"credibility\": 9,\\n    \"positivity\": 2\\n}\\n\\nLocal Event (2.2/10): \"Regional sports championship\"\\n{\\n    \"scale\": 2,\\n    \"impact\": 2,\\n    \"potential\": 1,\\n    \"legacy\": 1,\\n    \"novelty\": 2,\\n    \"credibility\": 8,\\n    \"positivity\": 8\\n}\\n\\nSignificance ranges:\\nHigh (6+): Major impact on human civilization\\nMedium (3-5): Regional/industry significance\\nLow (1-2): Local/minor impact\\nScore based on the following news articleuser\\n\\nURL: https://www.huffpost.com/entry/afghan-adjustment-act-congress_n_6324ad6ee4b027aa4065ebef\\nHeadline: Bill To Help Afghans Who Escaped Taliban Faces Long Odds In The Senate\\nContent: WASHINGTON — A new bipartisan bill to give Afghans in America a pathway to permanent residency stands a slim chance of winning enough Republican support to become law in the near future. The U.S. brought in tens of thousands of people as part of its hasty retreat from Afghanistan last year but has not said that they can stay. The Afghan Adjustment Act, introduced in August by Sen. Amy Klobuchar (D-Minn.) with three Republican co-sponsors, would create a path to permanent residency for more than 77,000 Afghans who have arrived in the U.S. since last summer.  “Giving our Afghan allies – many of whom worked alongside our U.S. Military – a chance to apply for permanent legal status is the right and necessary thing to do,” Klobuchar told HuffPost in a statement. “This bipartisan legislation will provide Afghans who submit to additional vetting a green card to live and work in the United States.” The bill would also expand eligibility for so-called special immigrant visas to other at-risk groups still in Afghanistan, such as certain former members of the Afghan armed forces. “These people have no place to go,” bill co-sponsor Sen. Lindsey Graham (R-S.C.) told HuffPost. “Their country has fallen into hell.”  But Graham acknowledged that it would be difficult to win over enough Republicans to get the bill through the Senate, saying his colleagues have “a legitimate concern” that Afghan nationals could pose a security risk. Sen. Rick Scott (R-Fla.), for instance, argued that President Joe Biden’s administration failed to vet the 80,000 Afghans admitted to the U.S. last year. A Defense Department report from February said its records on 50 Afghan evacuees indicated “potentially significant security concerns.” “I think we need to start by finding out who’s here and what their background is, and making sure people around the country know where these people went to,” Scott told HuffPost.  As November’s midterm elections approach, Republicans have been more hostile toward immigrants in general, with some describing them as invaders pouring through an insecure southern border. GOP governors have even sent buses and planes full of migrants to Democratic-led cities as a political stunt. In August 2021, the Biden administration used a measure known as humanitarian parole to allow Afghans to enter the U.S. for a two-year period. Officials ran evacuees’ fingerprints and records through criminal and terrorism databases at military installations in Europe and the Middle East. Parole is not recognized as an immigration status, and it offers no road to permanent residency. The main options for Afghans are asylum and special immigrant status, but these can be complicated and costly to attain.  The Biden administration announced in March that Afghans would be eligible for temporary protected status, preventing them from having to return to unsafe conditions, but TPS does not guarantee permanent residency. Advocates of the Afghan Adjustment Act — who point to the fact that Afghans in the U.S. could lose eligibility for humanitarian parole or temporary protected status next year and potentially face deportation — are optimistic about the bill’s prospects. “Even if we run out of time this month, we have other opportunities to get this done,” said Chris Purdy, the director of Veterans for American Ideals and Outreach at the nonprofit Human Rights First. “Passing this bill is a major priority for members in both houses. And every time we’ve brought this to Congress, we’ve gotten more and more supporters. Passage of the Afghan Adjustment Act is not a matter of if; it’s a matter of when.”  There has been less violence in Afghanistan since the U.S. finished its withdrawal last summer, but the ruling Taliban reportedly still target former members of the military and government officials, despite a declared amnesty. The government is accused of persecuting minority groups and imprisoning and torturing journalists, while women face restrictive new laws. The country is also experiencing an economic crisis, which is primarily the result of frozen assets in the United States, as well as other factors such as drought. Still, the near future is unlikely to see a critical mass of support for protecting Afghan evacuees, and Republicans could take control of one or both chambers of Congress next year, which would make this even more difficult. “I just wish we could all agree, conceptually, that the people who are here have no place to go back, and they were involved in a war that we were involved in for 20 years and how we treat them will say a lot about how people will fight with us in the future,” Graham said.  On Thursday, a small number of supporters rallied for the bill outside of Congress. “Many of those we are advocating for, if not all, have made the ultimate sacrifice by putting their lives in danger protecting democracy and helping our men and women in uniform,” Jawaid Kotwal, an Afghan American, told Huffpost. “We will continue to fight for their rights and interests, and we will continue to advocate for policies that promote a more just and equitable society,” Kotwal said.', 'system\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 24 Feb 2025\\n\\nReturn a score of each factor for the news article I have attached at the end (0-10 for each factor):\\n\\nFactors and weights:\\n- scale : Global population impact (10=affects all humanity, 5=regional, 2=local)\\n- impact : Immediate effect strength on humanity\\'s development\\n- potential : Future influence on human civilization\\n- legacy : Historical milestone for human progress\\n- novelty : Uniqueness in human history\\n- credibility : Source reliability\\n- positivity : Positive development for humanity\\n\\nScale scoring guideline:\\n10: Affects all of humanity directly\\n8: Major global impact\\n6: Multi-region impact\\n4: Regional impact\\n2: Local impact\\n\\nImpact scoring guideline:\\n10: Immediate global emergency\\n8: Major global change\\n6: Significant regional change\\n4: Moderate regional effect\\n2: Local effect\\n\\nReturn ONLY a JSON object with scores for the following news article. Use this format:\\n{\\n    \"scale\": N,\\n    \"impact\": N,\\n    \"potential\": N,\\n    \"legacy\": N,\\n    \"novelty\": N,\\n    \"credibility\": N,\\n    \"positivity\": N\\n}\\n\\n\\nCalibration Examples:\\nGlobal Impact (6.3/10): \"Earth surpasses 1.5°C warming limit\"\\n{\\n    \"scale\": 9,\\n    \"impact\": 8,\\n    \"potential\": 9,\\n    \"legacy\": 8,\\n    \"novelty\": 7,\\n    \"credibility\": 9,\\n    \"positivity\": 2\\n}\\n\\nRegional Conflict (5.0/10): \"Military escalation in ongoing war\"\\n{\\n    \"scale\": 5,\\n    \"impact\": 6,\\n    \"potential\": 5,\\n    \"legacy\": 5,\\n    \"novelty\": 5,\\n    \"credibility\": 9,\\n    \"positivity\": 2\\n}\\n\\nLocal Event (2.2/10): \"Regional sports championship\"\\n{\\n    \"scale\": 2,\\n    \"impact\": 2,\\n    \"potential\": 1,\\n    \"legacy\": 1,\\n    \"novelty\": 2,\\n    \"credibility\": 8,\\n    \"positivity\": 8\\n}\\n\\nSignificance ranges:\\nHigh (6+): Major impact on human civilization\\nMedium (3-5): Regional/industry significance\\nLow (1-2): Local/minor impact\\nScore based on the following news articleuser\\n\\nURL: https://www.huffpost.com/entry/capitol-riot-investigation-mark-meadows_n_63235733e4b000d988594a5d\\nHeadline: Mark Meadows Complies With Justice Dept. Subpoena: Report\\nContent:  WASHINGTON (AP) — Mark Meadows, the White House chief of staff under former President Donald Trump, has complied with a Justice Department subpoena and turned over records as part of a federal investigation into the Jan. 6, 2021 assault on the Capitol and efforts to overturn the 2020 presidential election, a person familiar the matter said Thursday. The records produced by Meadows are the same ones he earlier provided to a House committee conducting a similar investigation, according to the person, who spoke with The Associated Press on condition of anonymity to discuss an ongoing Justice Department probe.  The subpoena to Meadows, first reported by CNN, makes clear that Justice Department officials are seeking information from the most senior of Trump’s White House advisers as they examine wide-ranging efforts to overturn the results of the election won by Democrat Joe Biden. The department, whose work at times has mirrored or overlapped with that of the committee, this month served a broad wave of grand jury subpoenas and search warrants to Trump allies. Meadows has been a pivotal figure in the House investigation, his name invoked repeatedly in testimony by other Trump advisers, including by his own top aide. He had provided the committee with thousands of text messages, including communications with outside Trump allies and advisers.  In a filing in April in a federal lawsuit over his House subpoena, a lawyer for Meadows accused the committee of trying to vilify him publicly, noting that all of the texts it had been provided had been disclosed to the news media. The committee declined at the time to respond to the accusation. Meadows did not provide to the committee records he believed were subject to claims of executive privilege and those documents were also not produced to the Justice Department. ___ Follow Eric Tucker on Twitter at http://www.twitter.com/etuckerAP ___\\n}\\n\\n```json\\n{\\n    \"scale\": 6,\\n    \"impact\": 10,\\n    \"potential\": 6,\\n    \"legacy\": 6,\\n    \"novelty\": 2,\\n    \"credibility\": 8']\n"
     ]
    }
   ],
   "source": [
    "decoded = tokenizer.batch_decode(out, skip_special_tokens=True)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "def extract_label(decoded_text: str) -> str:\n",
    "    \"\"\"Extract the label from model output text\"\"\"\n",
    "    try:\n",
    "        # Split at the start token\n",
    "        parts = decoded_text.split(\"<|start_header_id|>assistant<|end_header_id|>\")\n",
    "        if len(parts) < 2:\n",
    "            return None\n",
    "\n",
    "        # Get the part after the header\n",
    "        content = parts[1]\n",
    "\n",
    "        # Split at the end token and get the first part\n",
    "        label = content.split(\"<|eot_id|>\")[0].strip()\n",
    "\n",
    "        return label\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting label: {e}\")\n",
    "        return None\n",
    "\n",
    "# Use it on your list\n",
    "labels = []\n",
    "for text in decoded:\n",
    "    label = extract_label(text)\n",
    "    if label:\n",
    "        labels.append(label)\n",
    "\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removing ads and new lines\n"
     ]
    }
   ],
   "source": [
    "first_row = df.iloc[0]\n",
    "first_row\n",
    "PROMPT = [\n",
    "  {\n",
    "    \"role\" : \"system\",\n",
    "    \"content\" : SYSTEM_PROMPT[\"content\"]\n",
    "  },\n",
    "  {\n",
    "    \"role\" : \"user\",\n",
    "    \"content\": f\"URL: {first_row['url']}\\nHeadline: {first_row['title']}\\nContent: {NSC.extract_article_content(first_row['url']).get('text', '')}\"\n",
    "  }\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PROMPT' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mpad_token \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39meos_token\n\u001b[1;32m      2\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mpad_token_id \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39meos_token_id\n\u001b[1;32m      5\u001b[0m tokenised \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mapply_chat_template(\n\u001b[0;32m----> 6\u001b[0m   \u001b[43mPROMPT\u001b[49m,\n\u001b[1;32m      7\u001b[0m   continue_final_message\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      8\u001b[0m   padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      9\u001b[0m   return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     10\u001b[0m )\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     11\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m tokenised_output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     12\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m tokenised_output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PROMPT' is not defined"
     ]
    }
   ],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "\n",
    "tokenised = tokenizer.apply_chat_template(\n",
    "  PROMPT,\n",
    "  continue_final_message=True,\n",
    "  padding=True,\n",
    "  return_tensors=\"pt\"\n",
    ").to(device)\n",
    "input_ids = tokenised_output[\"input_ids\"].to(device)\n",
    "attention_mask = tokenised_output[\"attention_mask\"].to(device)\n",
    "\n",
    "out = model.generate(input_ids=input_ids, attention_mask=attention_mask,  max_new_tokens=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m decoded \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241m.\u001b[39mbatch_decode(out, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      2\u001b[0m decoded\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "decoded = tokenizer.batch_decode(out, skip_special_tokens=True)\n",
    "decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch as nn\n",
    "# the text\n",
    "print(torch.cuda.is_available())\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "text = \"Hello how are\"\n",
    "# tokenising the text\n",
    "input_ids = tokenizer([text], return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "# pass into model\n",
    "out = model(input_ids=input_ids, max_new_tokens=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[128000,   9906,   1268,    527]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#128000, 9906, 1268, 527 these are the tokens, 128000 is the beginning of a sentence, 9906 is the Hello etc\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/llmspace/lib/python3.9/site-packages/torch/nn/modules/module.py:1739: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "probabilitydist = nn.Softmax()(out.logits[0, -1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9883, dtype=torch.bfloat16, grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# probability of the next token being 450 from the model\n",
    "probabilitydist[499]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 128256])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unnormalised scores that the model produces as its confidence in what it thinks the next word will be\n",
    "out.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9.8750, 6.3125, 1.7266,  ..., 0.4023, 0.4023, 0.4004],\n",
       "       dtype=torch.bfloat16, grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.logits[0, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9514"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(9514)\n",
    "# there is a space before you and space after you\n",
    "# this calculates the token value of the word \"you\"\n",
    "tokenizer.vocab[\"you\"]\n",
    "tokenizer.vocab[\"you\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(499)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.logits.argmax(axis=-1)[0, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[128000,  29673,    311,  61577,   8304,    449,  12431,     33]])\n",
      "['<|begin_of_text|>Subscribe to Neural Network with AVB']\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[128000,  29673,    311,  61577,   8304,    449,  12431]])\n",
      "tensor([[29673,   311, 61577,  8304,   449, 12431,    33]])\n"
     ]
    }
   ],
   "source": [
    "# We input this into the transformer\n",
    "# Then it will apply its causual mask attention\n",
    "# And then the output token will be what it predicts as the next token\n",
    "# extract the input sequence and target sequence from the tokenised sequence\n",
    "input_ids = tokenised[:, :-1] # start to end - 1\n",
    "target_ids = tokenised[:, 1:] # start + 1 to end\n",
    "\n",
    "print(input_ids)\n",
    "print(target_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss functions\n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m sentence \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSubscribe to neural network break with AVB\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m----> 2\u001b[0m tokenized \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m(sentence, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokenized)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "sentence = [\"Subscribe to neural network break with AVB\"]\n",
    "tokenized = tokenizer(sentence, return_tensors=\"pt\")[\"input_ids\"]\n",
    "print(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[128000,  29673,    311,  61577,   8304,    449,  12431,     33]])\n",
      "['<|begin_of_text|>Subscribe to Neural Network with AVB']\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "sentence = [\"Subscribe to Neural Network with AVB\"]\n",
    "tokenised = tokenizer(sentence, return_tensors=\"pt\")[\"input_ids\"]\n",
    "print(tokenised)\n",
    "print(tokenizer.batch_decode(tokenised))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetuning sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[128000,  11907,  24808,   9499,   5376,    555,    220,     17,  12628]])\n",
      "['<|begin_of_text|>Global warming temperature increase by 2 degrees']\n"
     ]
    }
   ],
   "source": [
    "#tokenising the sentence\n",
    "sentence = [\"Global warming temperature increase by 2 degrees\"]\n",
    "tokenised = tokenizer(sentence, return_tensors=\"pt\")[\"input_ids\"]\n",
    "print(tokenised)\n",
    "print(tokenizer.batch_decode(tokenised))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[128000,  11907,  24808,   9499,   5376,    555,    220,     17]])\n",
      "tensor([[11907, 24808,  9499,  5376,   555,   220,    17, 12628]])\n"
     ]
    }
   ],
   "source": [
    "# extracting input and target from the sequence\n",
    "#input into transformer\n",
    "input_ids = tokenised[:, :-1]\n",
    "# apply causal mask attention\n",
    "# output token will be what it predicts as the next token\n",
    "target_ids = tokenised[:, 1:]\n",
    "\n",
    "print(input_ids)\n",
    "print(target_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 25 Feb 2025\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Capital of India?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Capital:\n"
     ]
    }
   ],
   "source": [
    "question = \"Capital of Delhi?\"\n",
    "answer = \"New Delhi\"\n",
    "\n",
    "prompt = [\n",
    "  {\"role\" : \"user\", \"content\": \"Capital of India?\"},\n",
    "  {\"role\" : \"assistant\", \"content\": \"Capital:\"}\n",
    "]\n",
    "\n",
    "chat_template = tokenizer.apply_chat_template(prompt, continue_final_message=True, tokenize=False)\n",
    "print(chat_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 25 Feb 2025\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Capital of India?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Capital: New Delhi<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "#the entire sequence that we use so the model can learn on its won\n",
    "full_response = chat_template + \" \" + answer + tokenizer.eos_token\n",
    "print(full_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[128000, 128006,   9125, 128007,    271,  38766,   1303,  33025,   2696,\n",
      "             25,   6790,    220,   2366,     18,    198,  15724,   2696,     25,\n",
      "            220,    914,  13806,    220,   2366,     20,    271, 128009, 128006,\n",
      "            882, 128007,    271,  64693,    315,   6890,     30, 128009, 128006,\n",
      "          78191, 128007,    271,  64693,     25,   1561,  22767, 128009]])\n"
     ]
    }
   ],
   "source": [
    "tokenised = tokenizer(full_response, return_tensors=\"pt\", add_special_tokens=False)[\"input_ids\"]\n",
    "print(tokenised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[128000, 128006,   9125, 128007,    271,  38766,   1303,  33025,   2696,\n",
      "             25,   6790,    220,   2366,     18,    198,  15724,   2696,     25,\n",
      "            220,    914,  13806,    220,   2366,     20,    271, 128009, 128006,\n",
      "            882, 128007,    271,  64693,    315,   6890,     30, 128009, 128006,\n",
      "          78191, 128007,    271,  64693,     25,   1561,  22767]])\n",
      "tensor([[128006,   9125, 128007,    271,  38766,   1303,  33025,   2696,     25,\n",
      "           6790,    220,   2366,     18,    198,  15724,   2696,     25,    220,\n",
      "            914,  13806,    220,   2366,     20,    271, 128009, 128006,    882,\n",
      "         128007,    271,  64693,    315,   6890,     30, 128009, 128006,  78191,\n",
      "         128007,    271,  64693,     25,   1561,  22767, 128009]])\n"
     ]
    }
   ],
   "source": [
    "#input and target shifted by one to calculate the loss\n",
    "input_ids = tokenised[:, :-1]\n",
    "target_ids = tokenised[:, 1:]\n",
    "\n",
    "print(input_ids)\n",
    "print(target_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1561, 22767]])\n"
     ]
    }
   ],
   "source": [
    "# So target is the full sequence with New Delhi, and we calcualte the loss between input\n",
    "# and the target, so the model can learn to predict the next token which is New Delhi\n",
    "# It calculates the loss which is the difference between the predicted token and the actual token\n",
    "# It then backpropagates the loss to the model\n",
    "# The model then updates its weights to reduce the loss\n",
    "# The model then predicts the next token\n",
    "# The model then calculates the loss\n",
    "# The model then backpropagates the loss\n",
    "# The model then updates its weights\n",
    "\n",
    "labels_tokenised = tokenizer([\" \" + answer], add_special_tokens=False, return_tensors=\"pt\")[\"input_ids\"]\n",
    "print(labels_tokenised)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_tokenised = tokenizer([\" \" + answer + tokenizer.eos_token],\n",
    "           add_special_tokens=False, return_tensors=\"pt\", padding=\"max_length\",\n",
    "           max_length=target_ids.shape[1])[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
       "         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
       "         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
       "         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
       "         128009, 128009, 128009, 128009,   1561,  22767, 128009]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We want padding, padding which will be replaced by the actual answer\n",
    "#So the answer is 1561, 22767 and 128009 and the rest is padding\n",
    "labels_tokenised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          1561, 22767,  -100]])\n"
     ]
    }
   ],
   "source": [
    "# We want to replace the padding with -100\n",
    "# this is recommended by PyTorch for the loss function\n",
    "# -100 is used to mask the padding\n",
    "# so the model does not learn from the padding\n",
    "# the model only learns from the actual tokens\n",
    "labels_tokenised_fixed = torch.where(labels_tokenised != tokenizer.pad_token_id, labels_tokenised, -100)\n",
    "print(labels_tokenised_fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[128000, 128006,   9125, 128007,    271,  38766,   1303,  33025,   2696,\n",
      "             25,   6790,    220,   2366,     18,    198,  15724,   2696,     25,\n",
      "            220,   1627,  13806,    220,   2366,     20,    271, 128009, 128006,\n",
      "            882, 128007,    271,  64693,    315,   6890,     30, 128009, 128006,\n",
      "          78191, 128007,    271,  64693,     25,  35812]]), 'attention_mask': tensor([[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True, False,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True, False,  True,  True,  True,  True,  True,\n",
      "          True,  True]]), 'labels': tensor([[  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100, 128009]])}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def generate_input_output_pair(prompt, target_response):\n",
    "    # Apply chat template\n",
    "    chat_template = tokenizer.apply_chat_template(prompt, continue_final_message=True, tokenize=False)\n",
    "    # Tokenize input sequence\n",
    "    input_sequence = tokenizer(chat_template, return_tensors=\"pt\", add_special_tokens=False)[\"input_ids\"]\n",
    "\n",
    "    # Tokenize target sequence\n",
    "    target_sequence = tokenizer(\" \" + target_response + tokenizer.eos_token,\n",
    "                                return_tensors=\"pt\", add_special_tokens=False)[\"input_ids\"]\n",
    "\n",
    "    # Get full sequence\n",
    "    full_sequence = torch.cat([input_sequence, target_sequence], dim=1)\n",
    "\n",
    "    # Create inputs (without the last token)\n",
    "    inputs = full_sequence[:, :-1]\n",
    "    attention_mask = inputs != tokenizer.pad_token_id\n",
    "\n",
    "    # Create labels: -100 for input tokens, actual ids for target tokens\n",
    "    labels = torch.full_like(inputs, -100)\n",
    "    target_len = target_sequence.shape[1]\n",
    "    labels[:, -target_len+1:] = full_sequence[:, -target_len:][:, 1:]  # Shift target right by 1\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": inputs,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }\n",
    "\n",
    "prompt = [\n",
    "  {\"role\" : \"user\", \"content\": \"Capital of India?\"},\n",
    "  {\"role\" : \"assistant\", \"content\": \"Capital:\"}\n",
    "]\n",
    "answer = \"Mumbai\"\n",
    "data = generate_input_output_pair(prompt, answer)\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[128000, 128006,   9125, 128007,    271,  38766,   1303,  33025,   2696,\n",
       "             25,   6790,    220,   2366,     18,    198,  15724,   2696,     25,\n",
       "            220,   1627,  13806,    220,   2366,     20,    271, 128009, 128006,\n",
       "            882, 128007,    271,  64693,    315,   6890,     30, 128009, 128006,\n",
       "          78191, 128007,    271,  64693,     25,  35812]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "           -100,   -100,   -100,   -100,   -100, 128009]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The labels are the actual tokens that the model should predict\n",
    "data[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(input_ids=data[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 42, 128256])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "           -100,   -100,   -100,   -100,   -100, 128009]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "def calculate_loss(logits, labels):\n",
    "  loss_fn = nn.CrossEntropyLoss(reduction='none')\n",
    "  cross_entropy_loss = loss_fn(logits.view(-1, logits.shape[-1]), labels.view(-1))\n",
    "  return cross_entropy_loss\n",
    "\n",
    "# loss = calculate_loss(out.logits, data[\"labels\"].to(device))\n",
    "# loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_prompt = [\n",
    "  {\n",
    "    \"role\" : \"user\",\n",
    "    \"content\": \"Score a impact of the news article out of 10 : Global Tmeperature Increase by 2 degrees\"\n",
    "  },\n",
    "  {\n",
    "    \"role\" : \"assistant\",\n",
    "    \"content\": \"Significance Score :\"\n",
    "  }\n",
    "\n",
    "]\n",
    "\n",
    "target_response = \"6.3/10\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "/opt/anaconda3/envs/llmspace/lib/python3.9/site-packages/transformers/generation/utils.py:2105: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on mps. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('mps') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Feb 2025\n",
      "\n",
      "user\n",
      "\n",
      "Score a impact of the news article out of 10 : Global Tmeperature Increase by 2 degreesassistant\n",
      "\n",
      "Significance Score : 7\n",
      "\n",
      "The news article reports on the global\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "test_tokenized = tokenizer.apply_chat_template(training_prompt, continue_final_message=True, return_tensors=\"pt\")\n",
    "test_out = model.generate(test_tokenized, max_new_tokens=10)\n",
    "print(tokenizer.batch_decode(test_out, skip_special_tokens=True)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 7.17 GB, other allocations: 1.54 GB, max allowed: 9.07 GB). Tried to allocate 501.00 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m out \u001b[38;5;241m=\u001b[39m model(input_ids\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     11\u001b[0m loss \u001b[38;5;241m=\u001b[39m calculate_loss(out\u001b[38;5;241m.\u001b[39mlogits, data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m---> 13\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m optimiser\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     15\u001b[0m optimiser\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llmspace/lib/python3.9/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llmspace/lib/python3.9/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llmspace/lib/python3.9/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: MPS backend out of memory (MPS allocated: 7.17 GB, other allocations: 1.54 GB, max allowed: 9.07 GB). Tried to allocate 501.00 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "#\n",
    "data = generate_input_output_pair(training_prompt, target_response)\n",
    "data[\"input_ids\"] = data[\"input_ids\"]\n",
    "data[\"labels\"]\n",
    "\n",
    "optimiser = AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n",
    "\n",
    "for _ in range(10):\n",
    "    out = model(input_ids=data[\"input_ids\"])\n",
    "    loss = calculate_loss(out.logits, data[\"labels\"]).mean()\n",
    "\n",
    "    loss.backward()\n",
    "    optimiser.step()\n",
    "    optimiser.zero_grad()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
